{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford Cars\n",
    "\n",
    "* Data set: https://ai.stanford.edu/~jkrause/cars/car_dataset.html\n",
    "* Related papers: http://cs231n.stanford.edu/reports/2015/pdfs/lediurfinal.pdf, http://noiselab.ucsd.edu/ECE228/Reports/Report17.pdf\n",
    "* Databricks notebook: https://demo.cloud.databricks.com/#notebook/4718421/command/4718433\n",
    "* Databricks email thread: https://groups.google.com/a/databricks.com/d/msgid/ml-sme/CA%2BUeztiEsUTm2xEZnBZp2DOgiWocCkJ%3DLNo6q1-Fn3%2BXdN4prQ%40mail.gmail.com?utm_medium=email&utm_source=footer\n",
    "\n",
    "\n",
    "### Solutions\n",
    "\n",
    "* 88% accuracy with resnet152 https://github.com/foamliu/Car-Recognition\n",
    "* Kaggle solution with 90% accuracy: https://www.kaggle.com/meaninglesslives/cars-eb0-keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "import os\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.layers.pooling import GlobalAveragePooling2D, AveragePooling2D\n",
    "from keras import applications  # these are the applications built into keras\n",
    "from keras_applications.resnet import ResNet152 # separate keras applications lib, seems more up to date\n",
    "keras.backend.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings + globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True if you need to convert the original images into the squashed 227x227 images.\n",
    "# If you already have the squashed 227x227 images in cars_train_227_227, no need to run this. \n",
    "do_image_preprocessing = False\n",
    "\n",
    "# Set to True if you want to enable the step that builds a conv net from scratch (as opposed to transfer\n",
    "# learning).  \n",
    "do_conv_net_from_scratch = False\n",
    "\n",
    "# Set to true if you want to train/test vgg16\n",
    "enable_vgg_16_training_testing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_exists(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise Exception(\"Could not find path: {}\".format(path))\n",
    "\n",
    "datadir = \"../datasets/StanfordCars\"\n",
    "cars_train_227_227 = os.path.join(datadir, \"cars_train_227_227\")\n",
    "cars_test_227_227 = os.path.join(datadir, \"cars_test_227_227\")\n",
    "ensure_exists(cars_train_227_227)\n",
    "ensure_exists(cars_test_227_227)\n",
    "\n",
    "# Annotations\n",
    "cars_meta = sio.loadmat(datadir + \"/cars_meta.mat\")\n",
    "cars_train = sio.loadmat(datadir + \"/cars_train_annos.mat\")\n",
    "cars_test = sio.loadmat(datadir + \"/cars_test_annos.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [None] # MatLab is 1-based, python 0-based\n",
    "classes += [c[0].item() for c in cars_meta[\"class_names\"][0]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class(car):\n",
    "    \"\"\"\n",
    "    Helper function to convert a raw \"car\" stored in matlab format into\n",
    "    a dictionary w/ named fields\n",
    "    \"\"\"\n",
    "    filename = car[5][0].item()\n",
    "    class_id = car[4][0][0].item()\n",
    "    bbox = {\n",
    "        \"x1\": car[0][0][0].item(),\n",
    "        \"y1\": car[1][0][0].item(),\n",
    "        \"x2\": car[2][0][0].item(),\n",
    "        \"y2\": car[3][0][0].item()\n",
    "    }\n",
    "    class_ = classes[car[4][0][0]]\n",
    "    return {\n",
    "        \"filename\":filename, \n",
    "        \"class_id\": class_id,\n",
    "        \"class\": class_, \n",
    "        \"bbox\": bbox\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filename': '00003.jpg',\n",
       " 'class_id': 91,\n",
       " 'class': 'Dodge Dakota Club Cab 2007',\n",
       " 'bbox': {'x1': 85, 'y1': 109, 'x2': 601, 'y2': 381}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_annotations = cars_train['annotations'][0]\n",
    "car = training_annotations[2]\n",
    "car_class = get_class(car)\n",
    "car_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filename': '00002.jpg',\n",
       " 'class_id': 103,\n",
       " 'class': 'Ferrari 458 Italia Convertible 2012',\n",
       " 'bbox': {'x1': 100, 'y1': 19, 'x2': 576, 'y2': 203}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_annotations = cars_test['annotations'][0]\n",
    "car = test_annotations[1]\n",
    "car_class = get_class(car)\n",
    "car_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image preprocessing pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop with boundary\n",
    "\n",
    "From the Lieu/Wang paper:\n",
    "\n",
    "> To preserve some context surrounding the cars, we expanded each bounding box by 16 pixels on each side before cropping\n",
    "\n",
    "### Resize to 227x227 square aspect ratio\n",
    "\n",
    "From the Lieu/Wang paper:\n",
    "\n",
    "\n",
    "> we resized each cropped image to a square aspect ratio and a resolution of 227x227\n",
    "as required by the models. After discussions with Krause, we decided to squash images without preserving their original aspect ratios instead of scaling and cropping the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_expand_bounding_box(car_class, source_dir):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given a car class:\n",
    "    \n",
    "    {'filename': '00003.jpg',\n",
    "     'class_id': 145,\n",
    "     'class': 'Jeep Patriot SUV 2012',\n",
    "     'bbox': {'x1': 51, 'y1': 105, 'x2': 968, 'y2': 659}}\n",
    "     \n",
    "    And an source and output directory, do the following:\n",
    "    \n",
    "    1. Calculate the expanded bounding box (should not go outside image border)\n",
    "    2. Crop the image with the expanding box\n",
    "    3. Return cropped image\n",
    "    \"\"\"\n",
    "    source_filename = \"{}/{}\".format(source_dir, car_class['filename'])\n",
    "    \n",
    "    if not os.path.exists(source_filename):\n",
    "        raise Exception(\"Could not find source image file: {}\".format(source_filename))\n",
    "        \n",
    "    source_img = cv2.imread(source_filename)\n",
    "    height, width, channels = source_img.shape\n",
    "    bbox_orig = car_class['bbox']\n",
    "    bbox = expand_bounding_box(bbox_orig, (width, height), 16)\n",
    "    cropped_img = source_img[bbox['y1']:bbox['y2'], bbox['x1']:bbox['x2']]\n",
    "    return cropped_img\n",
    "\n",
    "def expand_bounding_box(bounding_box, img_size, expand_pixels):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given a bounding box:\n",
    "    \n",
    "    {'x1': 51, 'y1': 105, 'x2': 968, 'y2': 659}\n",
    "    \n",
    "    an image size tuple (width, height) and a number of pixels to expand (expand_pixels param)\n",
    "    \n",
    "    Return a larger bounding box that still fits within the image bounds.\n",
    "    \n",
    "    \"\"\"\n",
    "    width, height = img_size\n",
    "    new_x1 = max(bounding_box['x1'] - expand_pixels,0)  # don't let the new_x1 go off left edge of image\n",
    "    new_x2 = min(bounding_box['x2'] + expand_pixels, width)  # don't let new_x2 go off right edge of image\n",
    "    new_y1 = max(bounding_box['y1'] - expand_pixels, 0)  # don't go off top edge of image\n",
    "    new_y2 = min(bounding_box['y2'] + expand_pixels, height)  # don't go off bottom edge of image\n",
    "    \n",
    "    return {\n",
    "        'x1': new_x1,\n",
    "        'y1': new_y1,\n",
    "        'x2': new_x2,\n",
    "        'y2': new_y2,\n",
    "    }\n",
    "\n",
    "\n",
    "def process_cars(cars, source_dir, result_directory_path):\n",
    "    \"\"\"\n",
    "    Loop over car_classes and write transformed image into result_directory_path\n",
    "    \"\"\"\n",
    "    for car in cars:\n",
    "        car_class = get_class(car)\n",
    "        print(\"car_class: {}\".format(car_class))\n",
    "        cropped_img = crop_expand_bounding_box(car_class, source_dir)\n",
    "        resized_img = cv2.resize(cropped_img, (227,227))\n",
    "        target_file = os.path.join(result_directory_path, car_class['filename'])\n",
    "        cv2.imwrite(target_file, resized_img)\n",
    "        \n",
    "def process_car():\n",
    "    source_dir = os.path.join(datadir, \"cars_test\")\n",
    "    cropped_img = crop_expand_bounding_box(car_class, source_dir)\n",
    "\n",
    "    img = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_image_preprocessing:\n",
    "    source_dir = os.path.join(datadir, \"cars_train\")\n",
    "    process_cars(training_annotations, source_dir, cars_train_227_227)\n",
    "    source_dir = os.path.join(datadir, \"cars_test\")\n",
    "    process_cars(test_annotations, source_dir, cars_test_227_227)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras ImageDataGenerator\n",
    "\n",
    "### Based on tutorials/docs\n",
    "\n",
    "* [Vijayabhaskar J's Tutorial on Keras flow_from_dataframe](https://medium.com/@vijayabhaskar96/tutorial-on-keras-flow-from-dataframe-1fd4493d237c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframes_from_annotations(cars):\n",
    "    \"\"\"\n",
    "    Given the annotations in matlab/octave format, create dataframes\n",
    "    \"\"\"\n",
    "    dataframe = pd.DataFrame(columns=['id', 'label'])\n",
    "    \n",
    "    for car in cars:\n",
    "        # Example car_class: {'filename': '00001.jpg', 'class_id': 14, 'class': 'Audi TTS Coupe 2012', 'bbox': {..}}\n",
    "        car_class = get_class(car)\n",
    "        dataframe = dataframe.append(\n",
    "            {\"id\": car_class['filename'], \n",
    "             \"label\": car_class['class'],\n",
    "            }, \n",
    "            ignore_index=True,\n",
    "        )\n",
    "    \n",
    "    return dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001.jpg</td>\n",
       "      <td>Audi TTS Coupe 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00002.jpg</td>\n",
       "      <td>Acura TL Sedan 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00003.jpg</td>\n",
       "      <td>Dodge Dakota Club Cab 2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00004.jpg</td>\n",
       "      <td>Hyundai Sonata Hybrid Sedan 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00005.jpg</td>\n",
       "      <td>Ford F-450 Super Duty Crew Cab 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8139</th>\n",
       "      <td>08140.jpg</td>\n",
       "      <td>Chrysler Town and Country Minivan 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8140</th>\n",
       "      <td>08141.jpg</td>\n",
       "      <td>smart fortwo Convertible 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8141</th>\n",
       "      <td>08142.jpg</td>\n",
       "      <td>Mercedes-Benz SL-Class Coupe 2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8142</th>\n",
       "      <td>08143.jpg</td>\n",
       "      <td>Ford GT Coupe 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8143</th>\n",
       "      <td>08144.jpg</td>\n",
       "      <td>Audi 100 Sedan 1994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8144 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                   label\n",
       "0     00001.jpg                     Audi TTS Coupe 2012\n",
       "1     00002.jpg                     Acura TL Sedan 2012\n",
       "2     00003.jpg              Dodge Dakota Club Cab 2007\n",
       "3     00004.jpg        Hyundai Sonata Hybrid Sedan 2012\n",
       "4     00005.jpg     Ford F-450 Super Duty Crew Cab 2012\n",
       "...         ...                                     ...\n",
       "8139  08140.jpg  Chrysler Town and Country Minivan 2012\n",
       "8140  08141.jpg           smart fortwo Convertible 2012\n",
       "8141  08142.jpg       Mercedes-Benz SL-Class Coupe 2009\n",
       "8142  08143.jpg                      Ford GT Coupe 2006\n",
       "8143  08144.jpg                     Audi 100 Sedan 1994\n",
       "\n",
       "[8144 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataframes = dataframes_from_annotations(training_annotations)\n",
    "training_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001.jpg</td>\n",
       "      <td>Suzuki Aerio Sedan 2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00002.jpg</td>\n",
       "      <td>Ferrari 458 Italia Convertible 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00003.jpg</td>\n",
       "      <td>Jeep Patriot SUV 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00004.jpg</td>\n",
       "      <td>Toyota Camry Sedan 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00005.jpg</td>\n",
       "      <td>Tesla Model S Sedan 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8036</th>\n",
       "      <td>08037.jpg</td>\n",
       "      <td>Chevrolet Sonic Sedan 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8037</th>\n",
       "      <td>08038.jpg</td>\n",
       "      <td>Audi V8 Sedan 1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8038</th>\n",
       "      <td>08039.jpg</td>\n",
       "      <td>Audi 100 Sedan 1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8039</th>\n",
       "      <td>08040.jpg</td>\n",
       "      <td>BMW Z4 Convertible 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8040</th>\n",
       "      <td>08041.jpg</td>\n",
       "      <td>BMW X5 SUV 2007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8041 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                label\n",
       "0     00001.jpg              Suzuki Aerio Sedan 2007\n",
       "1     00002.jpg  Ferrari 458 Italia Convertible 2012\n",
       "2     00003.jpg                Jeep Patriot SUV 2012\n",
       "3     00004.jpg              Toyota Camry Sedan 2012\n",
       "4     00005.jpg             Tesla Model S Sedan 2012\n",
       "...         ...                                  ...\n",
       "8036  08037.jpg           Chevrolet Sonic Sedan 2012\n",
       "8037  08038.jpg                   Audi V8 Sedan 1994\n",
       "8038  08039.jpg                  Audi 100 Sedan 1994\n",
       "8039  08040.jpg              BMW Z4 Convertible 2012\n",
       "8040  08041.jpg                      BMW X5 SUV 2007\n",
       "\n",
       "[8041 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataframes = dataframes_from_annotations(test_annotations)\n",
    "test_dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Baseline convnet from scratch\n",
    "\n",
    "This takes the same approach as https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html, but it's not working very well.\n",
    "\n",
    "TODO: checkout what they did differently in http://noiselab.ucsd.edu/ECE228/Reports/Report17.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/validation ImageDataGenerator helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_classes = 196 # the number of different cars\n",
    "img_width = 227\n",
    "img_height = 227\n",
    "\n",
    "# TODO: I don't know if I should rely on the ImageDataGenerator for\n",
    "# the validation split, since as seen in https://www.kaggle.com/meaninglesslives/cars-eb0-keras/notebook\n",
    "# and https://github.com/foamliu/Car-Recognition/blob/master/train.py, it does a lot of data \n",
    "# augmentation on the training set, but NOT on the validation set.  So maybe it would be better\n",
    "# to write the \n",
    "datagen=ImageDataGenerator(rescale=1./255.,validation_split=0.25)\n",
    "\n",
    "\n",
    "def get_train_generator(shuffle=False):\n",
    "    train_generator=datagen.flow_from_dataframe(\n",
    "        dataframe=training_dataframes,\n",
    "        directory=cars_train_227_227,\n",
    "        x_col='id',\n",
    "        y_col='label',\n",
    "        subset=\"training\",\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        seed=42,\n",
    "        class_mode=\"categorical\",\n",
    "        target_size=(img_width,img_height),\n",
    "    )\n",
    "    return train_generator\n",
    "\n",
    "def get_validation_generator(shuffle=False):\n",
    "    validation_generator=datagen.flow_from_dataframe(\n",
    "        dataframe=training_dataframes,\n",
    "        directory=cars_train_227_227,\n",
    "        x_col='id',\n",
    "        y_col='label',\n",
    "        subset=\"validation\",\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        seed=42,\n",
    "        class_mode=\"categorical\",\n",
    "        target_size=(img_width,img_height),\n",
    "    )\n",
    "    return validation_generator\n",
    "\n",
    "def get_test_generator(shuffle=False,classes=None):\n",
    "    \"\"\"\n",
    "    This must take the \"classes\" as a param, which is a list of all the class labels:\n",
    "    \n",
    "        ['Audi TTS Coupe 2012', 'Acura TL Sedan 2012']\n",
    "    \n",
    "    Where the order is very important, because it's used to generate the one-hot\n",
    "    encoded labels.  If the one-hot encoded labels are misaligned across the\n",
    "    DataFrameIterator (training, validation, and test) then you will get totally\n",
    "    wonky and invalid results.  This is required since the test set DataFrameIterators \n",
    "    uses it's own ImageDataGenerator separate from the one used by the training and \n",
    "    validation generators.\n",
    "    \"\"\"\n",
    "    test_datagen=ImageDataGenerator(rescale=1./255.)\n",
    "    test_generator=test_datagen.flow_from_dataframe(\n",
    "        dataframe=test_dataframes,\n",
    "        directory=cars_test_227_227,\n",
    "        x_col='id',\n",
    "        y_col='label',\n",
    "        classes=classes,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        seed=42,\n",
    "        class_mode=\"categorical\",\n",
    "        target_size=(img_width,img_height),\n",
    "    )\n",
    "    return test_generator\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate training/validation ImageDataGenerators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting train_generator_non_shuffle\n",
      "Found 6115 images belonging to 196 classes.\n",
      "getting train_generator\n",
      "Found 6115 images belonging to 196 classes.\n",
      "getting validation_generator_non_shuffle\n",
      "Found 2029 images belonging to 196 classes.\n",
      "getting validation_generator\n",
      "Found 2029 images belonging to 196 classes.\n",
      "getting test_generator\n",
      "Found 8041 images belonging to 196 classes.\n",
      "steps_per_epoch_training: 382\n",
      "steps_per_epoch_validation: 126\n"
     ]
    }
   ],
   "source": [
    "print(\"getting train_generator_non_shuffle\")\n",
    "train_generator_non_shuffle = get_train_generator(shuffle=False)\n",
    "print(\"getting train_generator\")\n",
    "train_generator = get_train_generator(shuffle=True)\n",
    "print(\"getting validation_generator_non_shuffle\")\n",
    "validation_generator_non_shuffle = get_validation_generator(shuffle=False)\n",
    "print(\"getting validation_generator\")\n",
    "validation_generator = get_validation_generator(shuffle=True)\n",
    "print(\"getting test_generator\")\n",
    "\n",
    "# Use the classes from any of the above DataFrameIterators for the\n",
    "# the test set DataFrameIterator.\n",
    "classes = list(train_generator_non_shuffle.class_indices.keys())\n",
    "test_generator = get_test_generator(shuffle=False, classes=classes)\n",
    "\n",
    "steps_per_epoch_training=train_generator_non_shuffle.n // train_generator_non_shuffle.batch_size\n",
    "steps_per_epoch_validation=validation_generator_non_shuffle.n // validation_generator_non_shuffle.batch_size\n",
    "steps_per_epoch_test=test_generator.n // test_generator.batch_size\n",
    "print(\"steps_per_epoch_training: {}\".format(steps_per_epoch_training))\n",
    "print(\"steps_per_epoch_validation: {}\".format(steps_per_epoch_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define convnet model\n",
    "\n",
    "This tries to build a convnet from scratch rather than using transfer learning to try to give some sort of baseline.  It's not giving any decent level of accuracy on the validation set though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Conv / pooling layers\n",
    "model_convnet = Sequential()\n",
    "model_convnet.add(Conv2D(32, (3, 3), input_shape=(img_width, img_height, 3)))\n",
    "model_convnet.add(Activation('relu'))\n",
    "model_convnet.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model_convnet.add(Conv2D(32, (3, 3)))\n",
    "model_convnet.add(Activation('relu'))\n",
    "model_convnet.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model_convnet.add(Conv2D(64, (3, 3)))\n",
    "model_convnet.add(Activation('relu'))\n",
    "model_convnet.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Fully connected layers\n",
    "\n",
    "model_convnet.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model_convnet.add(Dense(256))\n",
    "model_convnet.add(Activation('relu'))\n",
    "model_convnet.add(Dropout(0.5))\n",
    "model_convnet.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_convnet.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "if do_conv_net_from_scratch:\n",
    "    model_convnet.fit_generator(\n",
    "        generator=train_generator,\n",
    "        steps_per_epoch=steps_per_epoch_training,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=steps_per_epoch_validation,\n",
    "        epochs=num_epochs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras VGG16 transfer learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate bottleneck features\n",
    "\n",
    "See dogs_vs_cats.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 227, 227, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 227, 227, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 227, 227, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 113, 113, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 113, 113, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 113, 113, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build the network\n",
    "model_vgg16 = applications.VGG16(\n",
    "    weights='imagenet', \n",
    "    input_shape=(img_width, img_height, 3), \n",
    "    include_top=False\n",
    ")\n",
    "model_vgg16.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get bottleneck predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_with_labels(model, generator):\n",
    "    \"\"\"\n",
    "    Helper which is an alternative to using model.predict_generator() which \n",
    "    has the advantage of also capturing the labels.\n",
    "    See https://stackoverflow.com/questions/44970445/how-to-return-true-labels-of-items-when-using-predict-generator\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        x, y = generator.next()\n",
    "        yield x, model.predict_on_batch(x), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_last_cnn_layer_with_labels(model, image_data_generator, steps_per_epoch):\n",
    "        \n",
    "    image_data_generator_w_labels = generator_with_labels(\n",
    "        model, \n",
    "        image_data_generator,\n",
    "    )\n",
    "    \n",
    "    num_steps_taken = 0\n",
    "    y_preds = []\n",
    "    y_labels = []\n",
    "    for x, y_pred, y_label in image_data_generator_w_labels:\n",
    "        print(\"{}/{}\".format(num_steps_taken, steps_per_epoch))\n",
    "        y_preds.append(y_pred)\n",
    "        y_labels.append(y_label)\n",
    "        num_steps_taken += 1\n",
    "        if num_steps_taken >= steps_per_epoch:\n",
    "            break\n",
    "            \n",
    "    return y_preds, y_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_vgg_16_training_testing:\n",
    "    training_y_preds_vgg16, training_y_labels_vgg16 = training_last_cnn_layer_with_labels(\n",
    "        model_vgg16,\n",
    "        train_generator_non_shuffle,\n",
    "        steps_per_epoch_training\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_vgg_16_training_testing:\n",
    "    validation_y_preds_vgg16, validation_y_labels_vgg16 = training_last_cnn_layer_with_labels(\n",
    "        model_vgg16,\n",
    "        validation_generator_non_shuffle,\n",
    "        steps_per_epoch_validation\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_vgg_16_training_testing:\n",
    "    print(len(training_y_preds_vgg16))\n",
    "    training_y_pred_vgg16 = training_y_preds_vgg16[0]\n",
    "    training_y_pred_vgg16.shape\n",
    "    training_y_preds_vgg16_array = np.array(training_y_preds_vgg16)\n",
    "    print(training_y_preds_vgg16_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_vgg_16_training_testing:\n",
    "    training_y_preds_flat_vgg16 = np.array(training_y_preds_vgg16).reshape(-1, 7, 7, 512)\n",
    "    training_y_labels_flat_vgg16 = np.array(training_y_labels_vgg16).reshape(-1, 196)\n",
    "    validation_y_preds_flat_vgg16 = np.array(validation_y_preds_vgg16).reshape(-1, 7, 7, 512)\n",
    "    validation_y_labels_flat_vgg16 = np.array(validation_y_labels_vgg16).reshape(-1, 196)\n",
    "    training_y_preds_flat_vgg16.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train top fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/fchollet/deep-learning-models/issues/13\n",
    "sgd = keras.optimizers.SGD(lr=0.0005, decay=1e-6, momentum=0.9)\n",
    "    \n",
    "def train_top_model(num_epochs, bottleneck_predictions_train, bottleneck_predictions_validation, train_labels, validation_labels):\n",
    "    \"\"\"\n",
    "    Best params so far:\n",
    "    \n",
    "    SGD with\n",
    "       - 2 4096 dense layers\n",
    "       - Dropout 0.6\n",
    "       - lr=0.0005, decay=1e-6, momentum=0.9\n",
    "       result: loss: 0.1307 - acc: 0.9890 - val_loss: 1.7402 - val_acc: 0.5342\n",
    "    \"\"\"\n",
    "    \n",
    "    top_model = Sequential()\n",
    "    top_model.add(Flatten(input_shape=bottleneck_predictions_train.shape[1:]))\n",
    "    top_model.add(Dense(4096, activation='relu'))\n",
    "    top_model.add(Dense(4096, activation='relu'))\n",
    "    top_model.add(Dropout(0.75))\n",
    "    top_model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    top_model.compile(\n",
    "        optimizer=sgd,\n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "\n",
    "    top_model.fit(bottleneck_predictions_train, \n",
    "              train_labels,\n",
    "              epochs=num_epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(bottleneck_predictions_validation, validation_labels))\n",
    "    \n",
    "    return top_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_vgg_16_training_testing:\n",
    "    top_model_vgg16 = train_top_model(\n",
    "        num_epochs=130,\n",
    "        bottleneck_predictions_train=training_y_preds_flat_vgg16,\n",
    "        bottleneck_predictions_validation=validation_y_preds_flat_vgg16,\n",
    "        train_labels=training_y_labels_flat_vgg16,\n",
    "        validation_labels=validation_y_labels_flat_vgg16,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set additional params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning_rate = 1e-4\n",
    "#momentum = 0.9\n",
    "\n",
    "# this is the number of layers that contains the first 4 (of 5 total) convblocks.\n",
    "# this is special because these are the layers that we will freeze, whereas convblock 5\n",
    "# will be fine-tuned.\n",
    "num_first_4_convblock_layers = 15 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate base model and freeze first four conv blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 227, 227, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 227, 227, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 227, 227, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 113, 113, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 113, 113, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 113, 113, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model_vgg16 = model_vgg16\n",
    "\n",
    "base_model_vgg16.summary()\n",
    "\n",
    "# Freeze first four conv blocks\n",
    "for layer in base_model_vgg16.layers[:num_first_4_convblock_layers]:\n",
    "    layer.trainable = False  # aka \"freeze\" this layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine base model with previously trained top model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_vgg_16_training_testing:\n",
    "    # add the model on top of the convolutional base\n",
    "    combined_model_vgg16 = keras.Model(\n",
    "        input= base_model_vgg16.input, \n",
    "        output=top_model_vgg16(base_model_vgg16.output)\n",
    "    )\n",
    "    combined_model_vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_vgg_16_training_testing:\n",
    "    combined_model_vgg16.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_vgg_16_training_testing:\n",
    "    combined_model_vgg16.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=steps_per_epoch_training,\n",
    "        epochs=5,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=steps_per_epoch_validation)\n",
    "    combined_model_vgg16.save(\"vgg16_fine_tuned_60percent_validation_accuracy.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set evaluation of fine-tuned VGG16 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_vgg_16_training_testing:\n",
    "    test_loss, test_accuracy = combined_model_vgg16.evaluate_generator(\n",
    "        generator = test_generator,\n",
    "        steps = steps_per_epoch_test,\n",
    "        verbose = 1,\n",
    "    )\n",
    "    print(\"test_loss: {}, test_accuracy: {}\".format(test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning on resnet-156\n",
    "\n",
    "According to http://noiselab.ucsd.edu/ECE228/Reports/Report17.pdf, they were only able to get ~50% test set accuracy on VGG16, which is probably comparable to the above model.\n",
    "\n",
    "TODO: first try to repro work from https://github.com/foamliu/Car-Recognition, then loop back to this.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate training/validation ImageDataGenerators\n",
    "\n",
    "Cannot re-use training generators from above, since they are already exhausted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting train_generator_non_shuffle\n",
      "Found 6115 images belonging to 196 classes.\n",
      "getting train_generator\n",
      "Found 6115 images belonging to 196 classes.\n",
      "getting validation_generator_non_shuffle\n",
      "Found 2029 images belonging to 196 classes.\n",
      "getting validation_generator\n",
      "Found 2029 images belonging to 196 classes.\n",
      "getting test_generator\n",
      "Found 8041 images belonging to 196 classes.\n",
      "steps_per_epoch_training: 382\n",
      "steps_per_epoch_validation: 126\n"
     ]
    }
   ],
   "source": [
    "print(\"getting train_generator_non_shuffle\")\n",
    "train_generator_non_shuffle = get_train_generator(shuffle=False)\n",
    "print(\"getting train_generator\")\n",
    "train_generator = get_train_generator(shuffle=True)\n",
    "print(\"getting validation_generator_non_shuffle\")\n",
    "validation_generator_non_shuffle = get_validation_generator(shuffle=False)\n",
    "print(\"getting validation_generator\")\n",
    "validation_generator = get_validation_generator(shuffle=True)\n",
    "print(\"getting test_generator\")\n",
    "\n",
    "# Use the classes from any of the above DataFrameIterators for the\n",
    "# the test set DataFrameIterator.\n",
    "classes = list(train_generator_non_shuffle.class_indices.keys())\n",
    "test_generator = get_test_generator(shuffle=False, classes=classes)\n",
    "\n",
    "steps_per_epoch_training=train_generator_non_shuffle.n // train_generator_non_shuffle.batch_size\n",
    "steps_per_epoch_validation=validation_generator_non_shuffle.n // validation_generator_non_shuffle.batch_size\n",
    "steps_per_epoch_test=test_generator.n // test_generator.batch_size\n",
    "print(\"steps_per_epoch_training: {}\".format(steps_per_epoch_training))\n",
    "print(\"steps_per_epoch_validation: {}\".format(steps_per_epoch_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define transfer learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 227, 227, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 233, 233, 3)  0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 114, 114, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 114, 114, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 114, 114, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 116, 116, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 57, 57, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 57, 57, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 57, 57, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 57, 57, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 57, 57, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 57, 57, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 57, 57, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 57, 57, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 57, 57, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 57, 57, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 57, 57, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 57, 57, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 57, 57, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 57, 57, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 57, 57, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 57, 57, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 57, 57, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 57, 57, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 57, 57, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 57, 57, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 57, 57, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 57, 57, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 57, 57, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 57, 57, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 57, 57, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 57, 57, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 57, 57, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 57, 57, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 57, 57, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 57, 57, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 57, 57, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 57, 57, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 57, 57, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 29, 29, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 29, 29, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 29, 29, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 29, 29, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 29, 29, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 29, 29, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 29, 29, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 29, 29, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 29, 29, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 29, 29, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 29, 29, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 29, 29, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 29, 29, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 29, 29, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 29, 29, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 29, 29, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 29, 29, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 29, 29, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 29, 29, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 29, 29, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 29, 29, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 29, 29, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 29, 29, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 29, 29, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 29, 29, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 29, 29, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 29, 29, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 29, 29, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 29, 29, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 29, 29, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 29, 29, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 29, 29, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 29, 29, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 29, 29, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 29, 29, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 29, 29, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 29, 29, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 29, 29, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 29, 29, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 29, 29, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 29, 29, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 29, 29, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_conv (Conv2D)    (None, 29, 29, 128)  65664       conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_bn (BatchNormali (None, 29, 29, 128)  512         conv3_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_relu (Activation (None, 29, 29, 128)  0           conv3_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_2_conv (Conv2D)    (None, 29, 29, 128)  147584      conv3_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_2_bn (BatchNormali (None, 29, 29, 128)  512         conv3_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_2_relu (Activation (None, 29, 29, 128)  0           conv3_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_3_conv (Conv2D)    (None, 29, 29, 512)  66048       conv3_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_3_bn (BatchNormali (None, 29, 29, 512)  2048        conv3_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_add (Add)          (None, 29, 29, 512)  0           conv3_block4_out[0][0]           \n",
      "                                                                 conv3_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_out (Activation)   (None, 29, 29, 512)  0           conv3_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_conv (Conv2D)    (None, 29, 29, 128)  65664       conv3_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_bn (BatchNormali (None, 29, 29, 128)  512         conv3_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_relu (Activation (None, 29, 29, 128)  0           conv3_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_2_conv (Conv2D)    (None, 29, 29, 128)  147584      conv3_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_2_bn (BatchNormali (None, 29, 29, 128)  512         conv3_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_2_relu (Activation (None, 29, 29, 128)  0           conv3_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_3_conv (Conv2D)    (None, 29, 29, 512)  66048       conv3_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_3_bn (BatchNormali (None, 29, 29, 512)  2048        conv3_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_add (Add)          (None, 29, 29, 512)  0           conv3_block5_out[0][0]           \n",
      "                                                                 conv3_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_out (Activation)   (None, 29, 29, 512)  0           conv3_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_conv (Conv2D)    (None, 29, 29, 128)  65664       conv3_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_bn (BatchNormali (None, 29, 29, 128)  512         conv3_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_relu (Activation (None, 29, 29, 128)  0           conv3_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_2_conv (Conv2D)    (None, 29, 29, 128)  147584      conv3_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_2_bn (BatchNormali (None, 29, 29, 128)  512         conv3_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_2_relu (Activation (None, 29, 29, 128)  0           conv3_block7_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_3_conv (Conv2D)    (None, 29, 29, 512)  66048       conv3_block7_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_3_bn (BatchNormali (None, 29, 29, 512)  2048        conv3_block7_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_add (Add)          (None, 29, 29, 512)  0           conv3_block6_out[0][0]           \n",
      "                                                                 conv3_block7_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_out (Activation)   (None, 29, 29, 512)  0           conv3_block7_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_conv (Conv2D)    (None, 29, 29, 128)  65664       conv3_block7_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_bn (BatchNormali (None, 29, 29, 128)  512         conv3_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_relu (Activation (None, 29, 29, 128)  0           conv3_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_2_conv (Conv2D)    (None, 29, 29, 128)  147584      conv3_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_2_bn (BatchNormali (None, 29, 29, 128)  512         conv3_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_2_relu (Activation (None, 29, 29, 128)  0           conv3_block8_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_3_conv (Conv2D)    (None, 29, 29, 512)  66048       conv3_block8_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_3_bn (BatchNormali (None, 29, 29, 512)  2048        conv3_block8_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_add (Add)          (None, 29, 29, 512)  0           conv3_block7_out[0][0]           \n",
      "                                                                 conv3_block8_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_out (Activation)   (None, 29, 29, 512)  0           conv3_block8_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 15, 15, 256)  131328      conv3_block8_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 15, 15, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 15, 15, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 15, 15, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 15, 15, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 15, 15, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 15, 15, 1024) 525312      conv3_block8_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 15, 15, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 15, 15, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 15, 15, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 15, 15, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 15, 15, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 15, 15, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 15, 15, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 15, 15, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 15, 15, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 15, 15, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 15, 15, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 15, 15, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 15, 15, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 15, 15, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 15, 15, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 15, 15, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 15, 15, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 15, 15, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 15, 15, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 15, 15, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 15, 15, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 15, 15, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 15, 15, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 15, 15, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 15, 15, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 15, 15, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 15, 15, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 15, 15, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 15, 15, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 15, 15, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 15, 15, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 15, 15, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 15, 15, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 15, 15, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 15, 15, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 15, 15, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 15, 15, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 15, 15, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 15, 15, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 15, 15, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 15, 15, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 15, 15, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 15, 15, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 15, 15, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 15, 15, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 15, 15, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 15, 15, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 15, 15, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 15, 15, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 15, 15, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 15, 15, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 15, 15, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 15, 15, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 15, 15, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 15, 15, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_conv (Conv2D)    (None, 15, 15, 256)  262400      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_bn (BatchNormali (None, 15, 15, 256)  1024        conv4_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_relu (Activation (None, 15, 15, 256)  0           conv4_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_conv (Conv2D)    (None, 15, 15, 256)  590080      conv4_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_bn (BatchNormali (None, 15, 15, 256)  1024        conv4_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_relu (Activation (None, 15, 15, 256)  0           conv4_block7_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_3_conv (Conv2D)    (None, 15, 15, 1024) 263168      conv4_block7_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_3_bn (BatchNormali (None, 15, 15, 1024) 4096        conv4_block7_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_add (Add)          (None, 15, 15, 1024) 0           conv4_block6_out[0][0]           \n",
      "                                                                 conv4_block7_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_out (Activation)   (None, 15, 15, 1024) 0           conv4_block7_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_conv (Conv2D)    (None, 15, 15, 256)  262400      conv4_block7_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_bn (BatchNormali (None, 15, 15, 256)  1024        conv4_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_relu (Activation (None, 15, 15, 256)  0           conv4_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_conv (Conv2D)    (None, 15, 15, 256)  590080      conv4_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_bn (BatchNormali (None, 15, 15, 256)  1024        conv4_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_relu (Activation (None, 15, 15, 256)  0           conv4_block8_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_3_conv (Conv2D)    (None, 15, 15, 1024) 263168      conv4_block8_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_3_bn (BatchNormali (None, 15, 15, 1024) 4096        conv4_block8_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_add (Add)          (None, 15, 15, 1024) 0           conv4_block7_out[0][0]           \n",
      "                                                                 conv4_block8_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_out (Activation)   (None, 15, 15, 1024) 0           conv4_block8_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_conv (Conv2D)    (None, 15, 15, 256)  262400      conv4_block8_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_bn (BatchNormali (None, 15, 15, 256)  1024        conv4_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_relu (Activation (None, 15, 15, 256)  0           conv4_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_conv (Conv2D)    (None, 15, 15, 256)  590080      conv4_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_bn (BatchNormali (None, 15, 15, 256)  1024        conv4_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_relu (Activation (None, 15, 15, 256)  0           conv4_block9_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_3_conv (Conv2D)    (None, 15, 15, 1024) 263168      conv4_block9_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_3_bn (BatchNormali (None, 15, 15, 1024) 4096        conv4_block9_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_add (Add)          (None, 15, 15, 1024) 0           conv4_block8_out[0][0]           \n",
      "                                                                 conv4_block9_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_out (Activation)   (None, 15, 15, 1024) 0           conv4_block9_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_conv (Conv2D)   (None, 15, 15, 256)  262400      conv4_block9_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_relu (Activatio (None, 15, 15, 256)  0           conv4_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_conv (Conv2D)   (None, 15, 15, 256)  590080      conv4_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_relu (Activatio (None, 15, 15, 256)  0           conv4_block10_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_3_conv (Conv2D)   (None, 15, 15, 1024) 263168      conv4_block10_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_3_bn (BatchNormal (None, 15, 15, 1024) 4096        conv4_block10_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_add (Add)         (None, 15, 15, 1024) 0           conv4_block9_out[0][0]           \n",
      "                                                                 conv4_block10_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_out (Activation)  (None, 15, 15, 1024) 0           conv4_block10_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_conv (Conv2D)   (None, 15, 15, 256)  262400      conv4_block10_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_relu (Activatio (None, 15, 15, 256)  0           conv4_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_conv (Conv2D)   (None, 15, 15, 256)  590080      conv4_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_relu (Activatio (None, 15, 15, 256)  0           conv4_block11_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_3_conv (Conv2D)   (None, 15, 15, 1024) 263168      conv4_block11_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_3_bn (BatchNormal (None, 15, 15, 1024) 4096        conv4_block11_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_add (Add)         (None, 15, 15, 1024) 0           conv4_block10_out[0][0]          \n",
      "                                                                 conv4_block11_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_out (Activation)  (None, 15, 15, 1024) 0           conv4_block11_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_conv (Conv2D)   (None, 15, 15, 256)  262400      conv4_block11_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_relu (Activatio (None, 15, 15, 256)  0           conv4_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_conv (Conv2D)   (None, 15, 15, 256)  590080      conv4_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_relu (Activatio (None, 15, 15, 256)  0           conv4_block12_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_3_conv (Conv2D)   (None, 15, 15, 1024) 263168      conv4_block12_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_3_bn (BatchNormal (None, 15, 15, 1024) 4096        conv4_block12_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_add (Add)         (None, 15, 15, 1024) 0           conv4_block11_out[0][0]          \n",
      "                                                                 conv4_block12_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_out (Activation)  (None, 15, 15, 1024) 0           conv4_block12_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_conv (Conv2D)   (None, 15, 15, 256)  262400      conv4_block12_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_relu (Activatio (None, 15, 15, 256)  0           conv4_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_conv (Conv2D)   (None, 15, 15, 256)  590080      conv4_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_relu (Activatio (None, 15, 15, 256)  0           conv4_block13_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_3_conv (Conv2D)   (None, 15, 15, 1024) 263168      conv4_block13_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_3_bn (BatchNormal (None, 15, 15, 1024) 4096        conv4_block13_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_add (Add)         (None, 15, 15, 1024) 0           conv4_block12_out[0][0]          \n",
      "                                                                 conv4_block13_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_out (Activation)  (None, 15, 15, 1024) 0           conv4_block13_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_conv (Conv2D)   (None, 15, 15, 256)  262400      conv4_block13_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_relu (Activatio (None, 15, 15, 256)  0           conv4_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_conv (Conv2D)   (None, 15, 15, 256)  590080      conv4_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_relu (Activatio (None, 15, 15, 256)  0           conv4_block14_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_3_conv (Conv2D)   (None, 15, 15, 1024) 263168      conv4_block14_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_3_bn (BatchNormal (None, 15, 15, 1024) 4096        conv4_block14_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_add (Add)         (None, 15, 15, 1024) 0           conv4_block13_out[0][0]          \n",
      "                                                                 conv4_block14_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_out (Activation)  (None, 15, 15, 1024) 0           conv4_block14_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_conv (Conv2D)   (None, 15, 15, 256)  262400      conv4_block14_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_relu (Activatio (None, 15, 15, 256)  0           conv4_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_conv (Conv2D)   (None, 15, 15, 256)  590080      conv4_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_relu (Activatio (None, 15, 15, 256)  0           conv4_block15_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_3_conv (Conv2D)   (None, 15, 15, 1024) 263168      conv4_block15_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_3_bn (BatchNormal (None, 15, 15, 1024) 4096        conv4_block15_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_add (Add)         (None, 15, 15, 1024) 0           conv4_block14_out[0][0]          \n",
      "                                                                 conv4_block15_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_out (Activation)  (None, 15, 15, 1024) 0           conv4_block15_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_conv (Conv2D)   (None, 15, 15, 256)  262400      conv4_block15_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_relu (Activatio (None, 15, 15, 256)  0           conv4_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_conv (Conv2D)   (None, 15, 15, 256)  590080      conv4_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_relu (Activatio (None, 15, 15, 256)  0           conv4_block16_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_3_conv (Conv2D)   (None, 15, 15, 1024) 263168      conv4_block16_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_3_bn (BatchNormal (None, 15, 15, 1024) 4096        conv4_block16_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_add (Add)         (None, 15, 15, 1024) 0           conv4_block15_out[0][0]          \n",
      "                                                                 conv4_block16_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_out (Activation)  (None, 15, 15, 1024) 0           conv4_block16_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_conv (Conv2D)   (None, 15, 15, 256)  262400      conv4_block16_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block17_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_relu (Activatio (None, 15, 15, 256)  0           conv4_block17_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_conv (Conv2D)   (None, 15, 15, 256)  590080      conv4_block17_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block17_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_relu (Activatio (None, 15, 15, 256)  0           conv4_block17_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_3_conv (Conv2D)   (None, 15, 15, 1024) 263168      conv4_block17_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_3_bn (BatchNormal (None, 15, 15, 1024) 4096        conv4_block17_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_add (Add)         (None, 15, 15, 1024) 0           conv4_block16_out[0][0]          \n",
      "                                                                 conv4_block17_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_out (Activation)  (None, 15, 15, 1024) 0           conv4_block17_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_conv (Conv2D)   (None, 15, 15, 256)  262400      conv4_block17_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block18_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_relu (Activatio (None, 15, 15, 256)  0           conv4_block18_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_conv (Conv2D)   (None, 15, 15, 256)  590080      conv4_block18_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block18_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_relu (Activatio (None, 15, 15, 256)  0           conv4_block18_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_3_conv (Conv2D)   (None, 15, 15, 1024) 263168      conv4_block18_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_3_bn (BatchNormal (None, 15, 15, 1024) 4096        conv4_block18_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_add (Add)         (None, 15, 15, 1024) 0           conv4_block17_out[0][0]          \n",
      "                                                                 conv4_block18_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_out (Activation)  (None, 15, 15, 1024) 0           conv4_block18_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_conv (Conv2D)   (None, 15, 15, 256)  262400      conv4_block18_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block19_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_relu (Activatio (None, 15, 15, 256)  0           conv4_block19_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_conv (Conv2D)   (None, 15, 15, 256)  590080      conv4_block19_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block19_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_relu (Activatio (None, 15, 15, 256)  0           conv4_block19_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_3_conv (Conv2D)   (None, 15, 15, 1024) 263168      conv4_block19_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_3_bn (BatchNormal (None, 15, 15, 1024) 4096        conv4_block19_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_add (Add)         (None, 15, 15, 1024) 0           conv4_block18_out[0][0]          \n",
      "                                                                 conv4_block19_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_out (Activation)  (None, 15, 15, 1024) 0           conv4_block19_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_conv (Conv2D)   (None, 15, 15, 256)  262400      conv4_block19_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block20_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_relu (Activatio (None, 15, 15, 256)  0           conv4_block20_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_conv (Conv2D)   (None, 15, 15, 256)  590080      conv4_block20_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block20_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_relu (Activatio (None, 15, 15, 256)  0           conv4_block20_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_3_conv (Conv2D)   (None, 15, 15, 1024) 263168      conv4_block20_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_3_bn (BatchNormal (None, 15, 15, 1024) 4096        conv4_block20_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_add (Add)         (None, 15, 15, 1024) 0           conv4_block19_out[0][0]          \n",
      "                                                                 conv4_block20_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_out (Activation)  (None, 15, 15, 1024) 0           conv4_block20_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_conv (Conv2D)   (None, 15, 15, 256)  262400      conv4_block20_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block21_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_relu (Activatio (None, 15, 15, 256)  0           conv4_block21_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_conv (Conv2D)   (None, 15, 15, 256)  590080      conv4_block21_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block21_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_relu (Activatio (None, 15, 15, 256)  0           conv4_block21_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_3_conv (Conv2D)   (None, 15, 15, 1024) 263168      conv4_block21_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_3_bn (BatchNormal (None, 15, 15, 1024) 4096        conv4_block21_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_add (Add)         (None, 15, 15, 1024) 0           conv4_block20_out[0][0]          \n",
      "                                                                 conv4_block21_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_out (Activation)  (None, 15, 15, 1024) 0           conv4_block21_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_conv (Conv2D)   (None, 15, 15, 256)  262400      conv4_block21_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block22_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_relu (Activatio (None, 15, 15, 256)  0           conv4_block22_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_conv (Conv2D)   (None, 15, 15, 256)  590080      conv4_block22_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block22_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_relu (Activatio (None, 15, 15, 256)  0           conv4_block22_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_3_conv (Conv2D)   (None, 15, 15, 1024) 263168      conv4_block22_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_3_bn (BatchNormal (None, 15, 15, 1024) 4096        conv4_block22_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_add (Add)         (None, 15, 15, 1024) 0           conv4_block21_out[0][0]          \n",
      "                                                                 conv4_block22_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_out (Activation)  (None, 15, 15, 1024) 0           conv4_block22_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_conv (Conv2D)   (None, 15, 15, 256)  262400      conv4_block22_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block23_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_relu (Activatio (None, 15, 15, 256)  0           conv4_block23_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_conv (Conv2D)   (None, 15, 15, 256)  590080      conv4_block23_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block23_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_relu (Activatio (None, 15, 15, 256)  0           conv4_block23_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_3_conv (Conv2D)   (None, 15, 15, 1024) 263168      conv4_block23_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_3_bn (BatchNormal (None, 15, 15, 1024) 4096        conv4_block23_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_add (Add)         (None, 15, 15, 1024) 0           conv4_block22_out[0][0]          \n",
      "                                                                 conv4_block23_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_out (Activation)  (None, 15, 15, 1024) 0           conv4_block23_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_conv (Conv2D)   (None, 15, 15, 256)  262400      conv4_block23_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block24_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_relu (Activatio (None, 15, 15, 256)  0           conv4_block24_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_2_conv (Conv2D)   (None, 15, 15, 256)  590080      conv4_block24_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_2_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block24_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_2_relu (Activatio (None, 15, 15, 256)  0           conv4_block24_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_3_conv (Conv2D)   (None, 15, 15, 1024) 263168      conv4_block24_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_3_bn (BatchNormal (None, 15, 15, 1024) 4096        conv4_block24_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_add (Add)         (None, 15, 15, 1024) 0           conv4_block23_out[0][0]          \n",
      "                                                                 conv4_block24_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_out (Activation)  (None, 15, 15, 1024) 0           conv4_block24_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_1_conv (Conv2D)   (None, 15, 15, 256)  262400      conv4_block24_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_1_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block25_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_1_relu (Activatio (None, 15, 15, 256)  0           conv4_block25_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_2_conv (Conv2D)   (None, 15, 15, 256)  590080      conv4_block25_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_2_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block25_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_2_relu (Activatio (None, 15, 15, 256)  0           conv4_block25_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_3_conv (Conv2D)   (None, 15, 15, 1024) 263168      conv4_block25_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_3_bn (BatchNormal (None, 15, 15, 1024) 4096        conv4_block25_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_add (Add)         (None, 15, 15, 1024) 0           conv4_block24_out[0][0]          \n",
      "                                                                 conv4_block25_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_out (Activation)  (None, 15, 15, 1024) 0           conv4_block25_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_1_conv (Conv2D)   (None, 15, 15, 256)  262400      conv4_block25_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_1_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block26_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_1_relu (Activatio (None, 15, 15, 256)  0           conv4_block26_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_2_conv (Conv2D)   (None, 15, 15, 256)  590080      conv4_block26_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_2_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block26_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_2_relu (Activatio (None, 15, 15, 256)  0           conv4_block26_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_3_conv (Conv2D)   (None, 15, 15, 1024) 263168      conv4_block26_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_3_bn (BatchNormal (None, 15, 15, 1024) 4096        conv4_block26_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_add (Add)         (None, 15, 15, 1024) 0           conv4_block25_out[0][0]          \n",
      "                                                                 conv4_block26_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_out (Activation)  (None, 15, 15, 1024) 0           conv4_block26_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_1_conv (Conv2D)   (None, 15, 15, 256)  262400      conv4_block26_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_1_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block27_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_1_relu (Activatio (None, 15, 15, 256)  0           conv4_block27_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_2_conv (Conv2D)   (None, 15, 15, 256)  590080      conv4_block27_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_2_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block27_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_2_relu (Activatio (None, 15, 15, 256)  0           conv4_block27_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_3_conv (Conv2D)   (None, 15, 15, 1024) 263168      conv4_block27_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_3_bn (BatchNormal (None, 15, 15, 1024) 4096        conv4_block27_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_add (Add)         (None, 15, 15, 1024) 0           conv4_block26_out[0][0]          \n",
      "                                                                 conv4_block27_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_out (Activation)  (None, 15, 15, 1024) 0           conv4_block27_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_1_conv (Conv2D)   (None, 15, 15, 256)  262400      conv4_block27_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_1_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block28_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_1_relu (Activatio (None, 15, 15, 256)  0           conv4_block28_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_2_conv (Conv2D)   (None, 15, 15, 256)  590080      conv4_block28_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_2_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block28_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_2_relu (Activatio (None, 15, 15, 256)  0           conv4_block28_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_3_conv (Conv2D)   (None, 15, 15, 1024) 263168      conv4_block28_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_3_bn (BatchNormal (None, 15, 15, 1024) 4096        conv4_block28_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_add (Add)         (None, 15, 15, 1024) 0           conv4_block27_out[0][0]          \n",
      "                                                                 conv4_block28_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_out (Activation)  (None, 15, 15, 1024) 0           conv4_block28_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_1_conv (Conv2D)   (None, 15, 15, 256)  262400      conv4_block28_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_1_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block29_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_1_relu (Activatio (None, 15, 15, 256)  0           conv4_block29_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_2_conv (Conv2D)   (None, 15, 15, 256)  590080      conv4_block29_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_2_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block29_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_2_relu (Activatio (None, 15, 15, 256)  0           conv4_block29_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_3_conv (Conv2D)   (None, 15, 15, 1024) 263168      conv4_block29_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_3_bn (BatchNormal (None, 15, 15, 1024) 4096        conv4_block29_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_add (Add)         (None, 15, 15, 1024) 0           conv4_block28_out[0][0]          \n",
      "                                                                 conv4_block29_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_out (Activation)  (None, 15, 15, 1024) 0           conv4_block29_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_1_conv (Conv2D)   (None, 15, 15, 256)  262400      conv4_block29_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_1_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block30_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_1_relu (Activatio (None, 15, 15, 256)  0           conv4_block30_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_2_conv (Conv2D)   (None, 15, 15, 256)  590080      conv4_block30_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_2_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block30_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_2_relu (Activatio (None, 15, 15, 256)  0           conv4_block30_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_3_conv (Conv2D)   (None, 15, 15, 1024) 263168      conv4_block30_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_3_bn (BatchNormal (None, 15, 15, 1024) 4096        conv4_block30_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_add (Add)         (None, 15, 15, 1024) 0           conv4_block29_out[0][0]          \n",
      "                                                                 conv4_block30_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_out (Activation)  (None, 15, 15, 1024) 0           conv4_block30_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_1_conv (Conv2D)   (None, 15, 15, 256)  262400      conv4_block30_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_1_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block31_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_1_relu (Activatio (None, 15, 15, 256)  0           conv4_block31_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_2_conv (Conv2D)   (None, 15, 15, 256)  590080      conv4_block31_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_2_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block31_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_2_relu (Activatio (None, 15, 15, 256)  0           conv4_block31_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_3_conv (Conv2D)   (None, 15, 15, 1024) 263168      conv4_block31_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_3_bn (BatchNormal (None, 15, 15, 1024) 4096        conv4_block31_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_add (Add)         (None, 15, 15, 1024) 0           conv4_block30_out[0][0]          \n",
      "                                                                 conv4_block31_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_out (Activation)  (None, 15, 15, 1024) 0           conv4_block31_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_1_conv (Conv2D)   (None, 15, 15, 256)  262400      conv4_block31_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_1_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block32_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_1_relu (Activatio (None, 15, 15, 256)  0           conv4_block32_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_2_conv (Conv2D)   (None, 15, 15, 256)  590080      conv4_block32_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_2_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block32_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_2_relu (Activatio (None, 15, 15, 256)  0           conv4_block32_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_3_conv (Conv2D)   (None, 15, 15, 1024) 263168      conv4_block32_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_3_bn (BatchNormal (None, 15, 15, 1024) 4096        conv4_block32_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_add (Add)         (None, 15, 15, 1024) 0           conv4_block31_out[0][0]          \n",
      "                                                                 conv4_block32_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_out (Activation)  (None, 15, 15, 1024) 0           conv4_block32_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_1_conv (Conv2D)   (None, 15, 15, 256)  262400      conv4_block32_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_1_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block33_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_1_relu (Activatio (None, 15, 15, 256)  0           conv4_block33_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_2_conv (Conv2D)   (None, 15, 15, 256)  590080      conv4_block33_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_2_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block33_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_2_relu (Activatio (None, 15, 15, 256)  0           conv4_block33_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_3_conv (Conv2D)   (None, 15, 15, 1024) 263168      conv4_block33_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_3_bn (BatchNormal (None, 15, 15, 1024) 4096        conv4_block33_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_add (Add)         (None, 15, 15, 1024) 0           conv4_block32_out[0][0]          \n",
      "                                                                 conv4_block33_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_out (Activation)  (None, 15, 15, 1024) 0           conv4_block33_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_1_conv (Conv2D)   (None, 15, 15, 256)  262400      conv4_block33_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_1_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block34_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_1_relu (Activatio (None, 15, 15, 256)  0           conv4_block34_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_2_conv (Conv2D)   (None, 15, 15, 256)  590080      conv4_block34_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_2_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block34_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_2_relu (Activatio (None, 15, 15, 256)  0           conv4_block34_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_3_conv (Conv2D)   (None, 15, 15, 1024) 263168      conv4_block34_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_3_bn (BatchNormal (None, 15, 15, 1024) 4096        conv4_block34_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_add (Add)         (None, 15, 15, 1024) 0           conv4_block33_out[0][0]          \n",
      "                                                                 conv4_block34_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_out (Activation)  (None, 15, 15, 1024) 0           conv4_block34_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_1_conv (Conv2D)   (None, 15, 15, 256)  262400      conv4_block34_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_1_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block35_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_1_relu (Activatio (None, 15, 15, 256)  0           conv4_block35_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_2_conv (Conv2D)   (None, 15, 15, 256)  590080      conv4_block35_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_2_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block35_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_2_relu (Activatio (None, 15, 15, 256)  0           conv4_block35_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_3_conv (Conv2D)   (None, 15, 15, 1024) 263168      conv4_block35_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_3_bn (BatchNormal (None, 15, 15, 1024) 4096        conv4_block35_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_add (Add)         (None, 15, 15, 1024) 0           conv4_block34_out[0][0]          \n",
      "                                                                 conv4_block35_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_out (Activation)  (None, 15, 15, 1024) 0           conv4_block35_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_1_conv (Conv2D)   (None, 15, 15, 256)  262400      conv4_block35_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_1_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block36_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_1_relu (Activatio (None, 15, 15, 256)  0           conv4_block36_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_2_conv (Conv2D)   (None, 15, 15, 256)  590080      conv4_block36_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_2_bn (BatchNormal (None, 15, 15, 256)  1024        conv4_block36_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_2_relu (Activatio (None, 15, 15, 256)  0           conv4_block36_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_3_conv (Conv2D)   (None, 15, 15, 1024) 263168      conv4_block36_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_3_bn (BatchNormal (None, 15, 15, 1024) 4096        conv4_block36_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_add (Add)         (None, 15, 15, 1024) 0           conv4_block35_out[0][0]          \n",
      "                                                                 conv4_block36_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_out (Activation)  (None, 15, 15, 1024) 0           conv4_block36_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 8, 8, 512)    524800      conv4_block36_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 8, 8, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 8, 8, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 8, 8, 2048)   2099200     conv4_block36_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 8, 8, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 8, 8, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 8, 8, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 8, 8, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 8, 8, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 8, 8, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 8, 8, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 8, 8, 2048)   0           conv5_block3_add[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 58,370,944\n",
      "Trainable params: 58,219,520\n",
      "Non-trainable params: 151,424\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build the network\n",
    "base_model_resnet152 = ResNet152(\n",
    "    weights='imagenet', \n",
    "    input_shape=(img_width, img_height, 3), \n",
    "    include_top=False,\n",
    "    backend=keras.backend,  # workaround keras issue: https://github.com/keras-team/keras-applications/issues/54#issuecomment-445097297\n",
    "    layers=keras.layers, \n",
    "    models=keras.models, \n",
    "    utils=keras.utils,\n",
    ")\n",
    "\n",
    "base_model_resnet152.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bottleneck features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/382\n",
      "1/382\n",
      "2/382\n",
      "3/382\n",
      "4/382\n",
      "5/382\n",
      "6/382\n",
      "7/382\n",
      "8/382\n",
      "9/382\n",
      "10/382\n",
      "11/382\n",
      "12/382\n",
      "13/382\n",
      "14/382\n",
      "15/382\n",
      "16/382\n",
      "17/382\n",
      "18/382\n",
      "19/382\n",
      "20/382\n",
      "21/382\n",
      "22/382\n",
      "23/382\n",
      "24/382\n",
      "25/382\n",
      "26/382\n",
      "27/382\n",
      "28/382\n",
      "29/382\n",
      "30/382\n",
      "31/382\n",
      "32/382\n",
      "33/382\n",
      "34/382\n",
      "35/382\n",
      "36/382\n",
      "37/382\n",
      "38/382\n",
      "39/382\n",
      "40/382\n",
      "41/382\n",
      "42/382\n",
      "43/382\n",
      "44/382\n",
      "45/382\n",
      "46/382\n",
      "47/382\n",
      "48/382\n",
      "49/382\n",
      "50/382\n",
      "51/382\n",
      "52/382\n",
      "53/382\n",
      "54/382\n",
      "55/382\n",
      "56/382\n",
      "57/382\n",
      "58/382\n",
      "59/382\n",
      "60/382\n",
      "61/382\n",
      "62/382\n",
      "63/382\n",
      "64/382\n",
      "65/382\n",
      "66/382\n",
      "67/382\n",
      "68/382\n",
      "69/382\n",
      "70/382\n",
      "71/382\n",
      "72/382\n",
      "73/382\n",
      "74/382\n",
      "75/382\n",
      "76/382\n",
      "77/382\n",
      "78/382\n",
      "79/382\n",
      "80/382\n",
      "81/382\n",
      "82/382\n",
      "83/382\n",
      "84/382\n",
      "85/382\n",
      "86/382\n",
      "87/382\n",
      "88/382\n",
      "89/382\n",
      "90/382\n",
      "91/382\n",
      "92/382\n",
      "93/382\n",
      "94/382\n",
      "95/382\n",
      "96/382\n",
      "97/382\n",
      "98/382\n",
      "99/382\n",
      "100/382\n",
      "101/382\n",
      "102/382\n",
      "103/382\n",
      "104/382\n",
      "105/382\n",
      "106/382\n",
      "107/382\n",
      "108/382\n",
      "109/382\n",
      "110/382\n",
      "111/382\n",
      "112/382\n",
      "113/382\n",
      "114/382\n",
      "115/382\n",
      "116/382\n",
      "117/382\n",
      "118/382\n",
      "119/382\n",
      "120/382\n",
      "121/382\n",
      "122/382\n",
      "123/382\n",
      "124/382\n",
      "125/382\n",
      "126/382\n",
      "127/382\n",
      "128/382\n",
      "129/382\n",
      "130/382\n",
      "131/382\n",
      "132/382\n",
      "133/382\n",
      "134/382\n",
      "135/382\n",
      "136/382\n",
      "137/382\n",
      "138/382\n",
      "139/382\n",
      "140/382\n",
      "141/382\n",
      "142/382\n",
      "143/382\n",
      "144/382\n",
      "145/382\n",
      "146/382\n",
      "147/382\n",
      "148/382\n",
      "149/382\n",
      "150/382\n",
      "151/382\n",
      "152/382\n",
      "153/382\n",
      "154/382\n",
      "155/382\n",
      "156/382\n",
      "157/382\n",
      "158/382\n",
      "159/382\n",
      "160/382\n",
      "161/382\n",
      "162/382\n",
      "163/382\n",
      "164/382\n",
      "165/382\n",
      "166/382\n",
      "167/382\n",
      "168/382\n",
      "169/382\n",
      "170/382\n",
      "171/382\n",
      "172/382\n",
      "173/382\n",
      "174/382\n",
      "175/382\n",
      "176/382\n",
      "177/382\n",
      "178/382\n",
      "179/382\n",
      "180/382\n",
      "181/382\n",
      "182/382\n",
      "183/382\n",
      "184/382\n",
      "185/382\n",
      "186/382\n",
      "187/382\n",
      "188/382\n",
      "189/382\n",
      "190/382\n",
      "191/382\n",
      "192/382\n",
      "193/382\n",
      "194/382\n",
      "195/382\n",
      "196/382\n",
      "197/382\n",
      "198/382\n",
      "199/382\n",
      "200/382\n",
      "201/382\n",
      "202/382\n",
      "203/382\n",
      "204/382\n",
      "205/382\n",
      "206/382\n",
      "207/382\n",
      "208/382\n",
      "209/382\n",
      "210/382\n",
      "211/382\n",
      "212/382\n",
      "213/382\n",
      "214/382\n",
      "215/382\n",
      "216/382\n",
      "217/382\n",
      "218/382\n",
      "219/382\n",
      "220/382\n",
      "221/382\n",
      "222/382\n",
      "223/382\n",
      "224/382\n",
      "225/382\n",
      "226/382\n",
      "227/382\n",
      "228/382\n",
      "229/382\n",
      "230/382\n",
      "231/382\n",
      "232/382\n",
      "233/382\n",
      "234/382\n",
      "235/382\n",
      "236/382\n",
      "237/382\n",
      "238/382\n",
      "239/382\n",
      "240/382\n",
      "241/382\n",
      "242/382\n",
      "243/382\n",
      "244/382\n",
      "245/382\n",
      "246/382\n",
      "247/382\n",
      "248/382\n",
      "249/382\n",
      "250/382\n",
      "251/382\n",
      "252/382\n",
      "253/382\n",
      "254/382\n",
      "255/382\n",
      "256/382\n",
      "257/382\n",
      "258/382\n",
      "259/382\n",
      "260/382\n",
      "261/382\n",
      "262/382\n",
      "263/382\n",
      "264/382\n",
      "265/382\n",
      "266/382\n",
      "267/382\n",
      "268/382\n",
      "269/382\n",
      "270/382\n",
      "271/382\n",
      "272/382\n",
      "273/382\n",
      "274/382\n",
      "275/382\n",
      "276/382\n",
      "277/382\n",
      "278/382\n",
      "279/382\n",
      "280/382\n",
      "281/382\n",
      "282/382\n",
      "283/382\n",
      "284/382\n",
      "285/382\n",
      "286/382\n",
      "287/382\n",
      "288/382\n",
      "289/382\n",
      "290/382\n",
      "291/382\n",
      "292/382\n",
      "293/382\n",
      "294/382\n",
      "295/382\n",
      "296/382\n",
      "297/382\n",
      "298/382\n",
      "299/382\n",
      "300/382\n",
      "301/382\n",
      "302/382\n",
      "303/382\n",
      "304/382\n",
      "305/382\n",
      "306/382\n",
      "307/382\n",
      "308/382\n",
      "309/382\n",
      "310/382\n",
      "311/382\n",
      "312/382\n",
      "313/382\n",
      "314/382\n",
      "315/382\n",
      "316/382\n",
      "317/382\n",
      "318/382\n",
      "319/382\n",
      "320/382\n",
      "321/382\n",
      "322/382\n",
      "323/382\n",
      "324/382\n",
      "325/382\n",
      "326/382\n",
      "327/382\n",
      "328/382\n",
      "329/382\n",
      "330/382\n",
      "331/382\n",
      "332/382\n",
      "333/382\n",
      "334/382\n",
      "335/382\n",
      "336/382\n",
      "337/382\n",
      "338/382\n",
      "339/382\n",
      "340/382\n",
      "341/382\n",
      "342/382\n",
      "343/382\n",
      "344/382\n",
      "345/382\n",
      "346/382\n",
      "347/382\n",
      "348/382\n",
      "349/382\n",
      "350/382\n",
      "351/382\n",
      "352/382\n",
      "353/382\n",
      "354/382\n",
      "355/382\n",
      "356/382\n",
      "357/382\n",
      "358/382\n",
      "359/382\n",
      "360/382\n",
      "361/382\n",
      "362/382\n",
      "363/382\n",
      "364/382\n",
      "365/382\n",
      "366/382\n",
      "367/382\n",
      "368/382\n",
      "369/382\n",
      "370/382\n",
      "371/382\n",
      "372/382\n",
      "373/382\n",
      "374/382\n",
      "375/382\n",
      "376/382\n",
      "377/382\n",
      "378/382\n",
      "379/382\n",
      "380/382\n",
      "381/382\n"
     ]
    }
   ],
   "source": [
    "training_y_preds_resnet152, training_y_labels_resnet152 = training_last_cnn_layer_with_labels(\n",
    "    base_model_resnet152,\n",
    "    train_generator_non_shuffle,\n",
    "    steps_per_epoch_training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/126\n",
      "1/126\n",
      "2/126\n",
      "3/126\n",
      "4/126\n",
      "5/126\n",
      "6/126\n",
      "7/126\n",
      "8/126\n",
      "9/126\n",
      "10/126\n",
      "11/126\n",
      "12/126\n",
      "13/126\n",
      "14/126\n",
      "15/126\n",
      "16/126\n",
      "17/126\n",
      "18/126\n",
      "19/126\n",
      "20/126\n",
      "21/126\n",
      "22/126\n",
      "23/126\n",
      "24/126\n",
      "25/126\n",
      "26/126\n",
      "27/126\n",
      "28/126\n",
      "29/126\n",
      "30/126\n",
      "31/126\n",
      "32/126\n",
      "33/126\n",
      "34/126\n",
      "35/126\n",
      "36/126\n",
      "37/126\n",
      "38/126\n",
      "39/126\n",
      "40/126\n",
      "41/126\n",
      "42/126\n",
      "43/126\n",
      "44/126\n",
      "45/126\n",
      "46/126\n",
      "47/126\n",
      "48/126\n",
      "49/126\n",
      "50/126\n",
      "51/126\n",
      "52/126\n",
      "53/126\n",
      "54/126\n",
      "55/126\n",
      "56/126\n",
      "57/126\n",
      "58/126\n",
      "59/126\n",
      "60/126\n",
      "61/126\n",
      "62/126\n",
      "63/126\n",
      "64/126\n",
      "65/126\n",
      "66/126\n",
      "67/126\n",
      "68/126\n",
      "69/126\n",
      "70/126\n",
      "71/126\n",
      "72/126\n",
      "73/126\n",
      "74/126\n",
      "75/126\n",
      "76/126\n",
      "77/126\n",
      "78/126\n",
      "79/126\n",
      "80/126\n",
      "81/126\n",
      "82/126\n",
      "83/126\n",
      "84/126\n",
      "85/126\n",
      "86/126\n",
      "87/126\n",
      "88/126\n",
      "89/126\n",
      "90/126\n",
      "91/126\n",
      "92/126\n",
      "93/126\n",
      "94/126\n",
      "95/126\n",
      "96/126\n",
      "97/126\n",
      "98/126\n",
      "99/126\n",
      "100/126\n",
      "101/126\n",
      "102/126\n",
      "103/126\n",
      "104/126\n",
      "105/126\n",
      "106/126\n",
      "107/126\n",
      "108/126\n",
      "109/126\n",
      "110/126\n",
      "111/126\n",
      "112/126\n",
      "113/126\n",
      "114/126\n",
      "115/126\n",
      "116/126\n",
      "117/126\n",
      "118/126\n",
      "119/126\n",
      "120/126\n",
      "121/126\n",
      "122/126\n",
      "123/126\n",
      "124/126\n",
      "125/126\n"
     ]
    }
   ],
   "source": [
    "validation_y_preds_resnet152, validation_y_labels_resnet152 = training_last_cnn_layer_with_labels(\n",
    "    base_model_resnet152,\n",
    "    validation_generator_non_shuffle,\n",
    "    steps_per_epoch_validation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382\n",
      "(382, 16, 8, 8, 2048)\n",
      "(6112, 8, 8, 2048)\n",
      "(8, 8, 2048)\n",
      "(126, 16, 8, 8, 2048)\n"
     ]
    }
   ],
   "source": [
    "print(len(training_y_preds_resnet152))\n",
    "training_y_pred_resnet152 = training_y_preds_resnet152[0]\n",
    "training_y_pred_resnet152.shape\n",
    "training_y_preds_resnet152_array = np.array(training_y_preds_resnet152)\n",
    "print(training_y_preds_resnet152_array.shape)\n",
    "training_y_preds_flat_resnet152 = np.array(training_y_preds_resnet152).reshape(-1, 8, 8, 2048)\n",
    "print(training_y_preds_flat_resnet152.shape)\n",
    "print(training_y_preds_flat_resnet152.shape[1:])\n",
    "print(np.array(validation_y_preds_resnet152).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_y_preds_flat_resnet152 = np.array(training_y_preds_resnet152).reshape(-1, 8, 8, 2048)\n",
    "training_y_labels_flat_resnet152 = np.array(training_y_labels_resnet152).reshape(-1, 196)\n",
    "validation_y_preds_flat_resnet152 = np.array(validation_y_preds_resnet152).reshape(-1, 8, 8, 2048)\n",
    "validation_y_labels_flat_resnet152 = np.array(validation_y_labels_resnet152).reshape(-1, 196)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6112 samples, validate on 2016 samples\n",
      "Epoch 1/500\n",
      "6112/6112 [==============================] - 10s 2ms/step - loss: 5.5876 - acc: 0.0047 - val_loss: 5.3771 - val_acc: 0.0040\n",
      "Epoch 2/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 5.4238 - acc: 0.0051 - val_loss: 5.3204 - val_acc: 0.0045\n",
      "Epoch 3/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 5.3523 - acc: 0.0059 - val_loss: 5.3192 - val_acc: 0.0089\n",
      "Epoch 4/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 5.3287 - acc: 0.0044 - val_loss: 5.3034 - val_acc: 0.0050\n",
      "Epoch 5/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 5.3048 - acc: 0.0044 - val_loss: 5.3001 - val_acc: 0.0030\n",
      "Epoch 6/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 5.2959 - acc: 0.0059 - val_loss: 5.2989 - val_acc: 0.0089\n",
      "Epoch 7/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 5.2890 - acc: 0.0065 - val_loss: 5.2949 - val_acc: 0.0089\n",
      "Epoch 8/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 5.2821 - acc: 0.0061 - val_loss: 5.2930 - val_acc: 0.0089\n",
      "Epoch 9/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 5.2816 - acc: 0.0083 - val_loss: 5.2947 - val_acc: 0.0089\n",
      "Epoch 10/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 5.2813 - acc: 0.0054 - val_loss: 5.2954 - val_acc: 0.0089\n",
      "Epoch 11/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 5.2794 - acc: 0.0080 - val_loss: 5.2948 - val_acc: 0.0089\n",
      "Epoch 12/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 5.2797 - acc: 0.0067 - val_loss: 5.2923 - val_acc: 0.0089\n",
      "Epoch 13/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 5.2754 - acc: 0.0079 - val_loss: 5.2925 - val_acc: 0.0089\n",
      "Epoch 14/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 5.2740 - acc: 0.0067 - val_loss: 5.2938 - val_acc: 0.0089\n",
      "Epoch 15/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 5.2725 - acc: 0.0079 - val_loss: 5.2962 - val_acc: 0.0094\n",
      "Epoch 16/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 5.2690 - acc: 0.0077 - val_loss: 5.2920 - val_acc: 0.0099\n",
      "Epoch 17/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 5.2628 - acc: 0.0072 - val_loss: 5.2796 - val_acc: 0.0089\n",
      "Epoch 18/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 5.2551 - acc: 0.0080 - val_loss: 5.2803 - val_acc: 0.0025\n",
      "Epoch 19/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 5.2444 - acc: 0.0097 - val_loss: 5.2753 - val_acc: 0.0084\n",
      "Epoch 20/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 5.2390 - acc: 0.0097 - val_loss: 5.2756 - val_acc: 0.0060\n",
      "Epoch 21/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 5.2330 - acc: 0.0110 - val_loss: 5.2732 - val_acc: 0.0045\n",
      "Epoch 22/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 5.2215 - acc: 0.0087 - val_loss: 5.2793 - val_acc: 0.0079\n",
      "Epoch 23/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 5.2199 - acc: 0.0090 - val_loss: 5.2736 - val_acc: 0.0089\n",
      "Epoch 24/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 5.2131 - acc: 0.0100 - val_loss: 5.2693 - val_acc: 0.0055\n",
      "Epoch 25/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 5.2064 - acc: 0.0098 - val_loss: 5.2660 - val_acc: 0.0099\n",
      "Epoch 26/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 5.1967 - acc: 0.0129 - val_loss: 5.2597 - val_acc: 0.0129\n",
      "Epoch 27/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 5.1935 - acc: 0.0103 - val_loss: 5.2589 - val_acc: 0.0114\n",
      "Epoch 28/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 5.1857 - acc: 0.0118 - val_loss: 5.2627 - val_acc: 0.0084\n",
      "Epoch 29/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 5.1790 - acc: 0.0141 - val_loss: 5.2598 - val_acc: 0.0114\n",
      "Epoch 30/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 5.1677 - acc: 0.0141 - val_loss: 5.2398 - val_acc: 0.0119\n",
      "Epoch 31/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 5.1517 - acc: 0.0162 - val_loss: 5.2281 - val_acc: 0.0114\n",
      "Epoch 32/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 5.1372 - acc: 0.0152 - val_loss: 5.2241 - val_acc: 0.0104\n",
      "Epoch 33/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 5.1090 - acc: 0.0185 - val_loss: 5.2047 - val_acc: 0.0144\n",
      "Epoch 34/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 5.0834 - acc: 0.0216 - val_loss: 5.1859 - val_acc: 0.0129\n",
      "Epoch 35/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 5.0516 - acc: 0.0213 - val_loss: 5.1726 - val_acc: 0.0124\n",
      "Epoch 36/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 5.0324 - acc: 0.0242 - val_loss: 5.1434 - val_acc: 0.0169\n",
      "Epoch 37/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 4.9993 - acc: 0.0236 - val_loss: 5.1758 - val_acc: 0.0119\n",
      "Epoch 38/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 4.9717 - acc: 0.0262 - val_loss: 5.1216 - val_acc: 0.0198\n",
      "Epoch 39/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 4.9539 - acc: 0.0286 - val_loss: 5.1085 - val_acc: 0.0139\n",
      "Epoch 40/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 4.9242 - acc: 0.0288 - val_loss: 5.1231 - val_acc: 0.0164\n",
      "Epoch 41/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 4.8990 - acc: 0.0314 - val_loss: 5.0772 - val_acc: 0.0218\n",
      "Epoch 42/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 4.8743 - acc: 0.0334 - val_loss: 5.0758 - val_acc: 0.0179\n",
      "Epoch 43/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 4.8485 - acc: 0.0375 - val_loss: 5.0566 - val_acc: 0.0164\n",
      "Epoch 44/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 4.8351 - acc: 0.0378 - val_loss: 5.0414 - val_acc: 0.0193\n",
      "Epoch 45/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 4.8068 - acc: 0.0402 - val_loss: 5.0498 - val_acc: 0.0223\n",
      "Epoch 46/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 4.7858 - acc: 0.0419 - val_loss: 5.0254 - val_acc: 0.0184\n",
      "Epoch 47/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 4.7570 - acc: 0.0448 - val_loss: 5.0049 - val_acc: 0.0198\n",
      "Epoch 48/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 4.7439 - acc: 0.0478 - val_loss: 5.0131 - val_acc: 0.0258\n",
      "Epoch 49/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 4.7044 - acc: 0.0491 - val_loss: 5.0137 - val_acc: 0.0228\n",
      "Epoch 50/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 4.6888 - acc: 0.0528 - val_loss: 4.9987 - val_acc: 0.0228\n",
      "Epoch 51/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 4.6579 - acc: 0.0528 - val_loss: 4.9689 - val_acc: 0.0228\n",
      "Epoch 52/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 4.6510 - acc: 0.0551 - val_loss: 4.9573 - val_acc: 0.0312\n",
      "Epoch 53/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 4.6076 - acc: 0.0596 - val_loss: 4.9790 - val_acc: 0.0238\n",
      "Epoch 54/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 4.5770 - acc: 0.0625 - val_loss: 4.9349 - val_acc: 0.0263\n",
      "Epoch 55/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 4.5506 - acc: 0.0674 - val_loss: 4.9306 - val_acc: 0.0317\n",
      "Epoch 56/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 4.5215 - acc: 0.0692 - val_loss: 4.9326 - val_acc: 0.0308\n",
      "Epoch 57/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 4.4956 - acc: 0.0715 - val_loss: 4.9243 - val_acc: 0.0367\n",
      "Epoch 58/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 4.4819 - acc: 0.0730 - val_loss: 4.9287 - val_acc: 0.0332\n",
      "Epoch 59/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 4.4506 - acc: 0.0722 - val_loss: 4.9097 - val_acc: 0.0342\n",
      "Epoch 60/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 4.4161 - acc: 0.0743 - val_loss: 4.9001 - val_acc: 0.0317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 4.3900 - acc: 0.0846 - val_loss: 4.9238 - val_acc: 0.0308\n",
      "Epoch 62/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 4.3625 - acc: 0.0831 - val_loss: 4.8870 - val_acc: 0.0407\n",
      "Epoch 63/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 4.3382 - acc: 0.0888 - val_loss: 4.9150 - val_acc: 0.0357\n",
      "Epoch 64/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 4.3242 - acc: 0.0897 - val_loss: 4.8970 - val_acc: 0.0327\n",
      "Epoch 65/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 4.2839 - acc: 0.0900 - val_loss: 4.8795 - val_acc: 0.0372\n",
      "Epoch 66/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 4.2735 - acc: 0.0964 - val_loss: 4.8919 - val_acc: 0.0372\n",
      "Epoch 67/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 4.2388 - acc: 0.1006 - val_loss: 4.9139 - val_acc: 0.0327\n",
      "Epoch 68/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 4.2057 - acc: 0.1047 - val_loss: 4.8727 - val_acc: 0.0357\n",
      "Epoch 69/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 4.1706 - acc: 0.1083 - val_loss: 4.8819 - val_acc: 0.0382\n",
      "Epoch 70/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 4.1519 - acc: 0.1121 - val_loss: 4.8565 - val_acc: 0.0397\n",
      "Epoch 71/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 4.1259 - acc: 0.1137 - val_loss: 4.8626 - val_acc: 0.0412\n",
      "Epoch 72/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 4.1100 - acc: 0.1140 - val_loss: 4.8613 - val_acc: 0.0392\n",
      "Epoch 73/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 4.0738 - acc: 0.1176 - val_loss: 4.8640 - val_acc: 0.0402\n",
      "Epoch 74/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 4.0431 - acc: 0.1268 - val_loss: 4.8580 - val_acc: 0.0422\n",
      "Epoch 75/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 4.0178 - acc: 0.1294 - val_loss: 4.8594 - val_acc: 0.0466\n",
      "Epoch 76/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 4.0141 - acc: 0.1294 - val_loss: 4.8997 - val_acc: 0.0407\n",
      "Epoch 77/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 3.9810 - acc: 0.1343 - val_loss: 4.8618 - val_acc: 0.0437\n",
      "Epoch 78/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 3.9517 - acc: 0.1369 - val_loss: 4.8876 - val_acc: 0.0441\n",
      "Epoch 79/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 3.9198 - acc: 0.1445 - val_loss: 4.8490 - val_acc: 0.0481\n",
      "Epoch 80/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 3.9048 - acc: 0.1414 - val_loss: 4.8389 - val_acc: 0.0471\n",
      "Epoch 81/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 3.8801 - acc: 0.1420 - val_loss: 4.8398 - val_acc: 0.0521\n",
      "Epoch 82/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 3.8564 - acc: 0.1556 - val_loss: 4.9603 - val_acc: 0.0412\n",
      "Epoch 83/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 3.8242 - acc: 0.1566 - val_loss: 4.8638 - val_acc: 0.0432\n",
      "Epoch 84/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 3.8031 - acc: 0.1531 - val_loss: 4.8690 - val_acc: 0.0471\n",
      "Epoch 85/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 3.7816 - acc: 0.1574 - val_loss: 4.8649 - val_acc: 0.0466\n",
      "Epoch 86/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 3.7645 - acc: 0.1626 - val_loss: 4.9068 - val_acc: 0.0466\n",
      "Epoch 87/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 3.7325 - acc: 0.1726 - val_loss: 4.8876 - val_acc: 0.0526\n",
      "Epoch 88/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 3.7086 - acc: 0.1716 - val_loss: 4.8785 - val_acc: 0.0486\n",
      "Epoch 89/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 3.6731 - acc: 0.1790 - val_loss: 4.8390 - val_acc: 0.0541\n",
      "Epoch 90/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 3.6513 - acc: 0.1844 - val_loss: 4.8656 - val_acc: 0.0486\n",
      "Epoch 91/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 3.6187 - acc: 0.1903 - val_loss: 4.8453 - val_acc: 0.0516\n",
      "Epoch 92/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 3.5948 - acc: 0.1939 - val_loss: 4.8920 - val_acc: 0.0496\n",
      "Epoch 93/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 3.5880 - acc: 0.1922 - val_loss: 4.8773 - val_acc: 0.0506\n",
      "Epoch 94/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 3.5607 - acc: 0.1990 - val_loss: 4.8693 - val_acc: 0.0526\n",
      "Epoch 95/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 3.5071 - acc: 0.2181 - val_loss: 4.9114 - val_acc: 0.0506\n",
      "Epoch 96/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 3.5091 - acc: 0.2030 - val_loss: 4.8885 - val_acc: 0.0471\n",
      "Epoch 97/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 3.4824 - acc: 0.2088 - val_loss: 4.9210 - val_acc: 0.0536\n",
      "Epoch 98/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 3.4503 - acc: 0.2184 - val_loss: 4.8942 - val_acc: 0.0496\n",
      "Epoch 99/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 3.4483 - acc: 0.2161 - val_loss: 4.8686 - val_acc: 0.0511\n",
      "Epoch 100/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 3.4139 - acc: 0.2258 - val_loss: 4.8891 - val_acc: 0.0511\n",
      "Epoch 101/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 3.4002 - acc: 0.2276 - val_loss: 4.9080 - val_acc: 0.0541\n",
      "Epoch 102/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 3.3661 - acc: 0.2323 - val_loss: 4.9297 - val_acc: 0.0491\n",
      "Epoch 103/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 3.3451 - acc: 0.2384 - val_loss: 4.9230 - val_acc: 0.0531\n",
      "Epoch 104/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 3.3301 - acc: 0.2354 - val_loss: 4.8856 - val_acc: 0.0585\n",
      "Epoch 105/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 3.2908 - acc: 0.2436 - val_loss: 4.9014 - val_acc: 0.0546\n",
      "Epoch 106/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 3.2746 - acc: 0.2552 - val_loss: 4.8775 - val_acc: 0.0556\n",
      "Epoch 107/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 3.2338 - acc: 0.2590 - val_loss: 5.0133 - val_acc: 0.0531\n",
      "Epoch 108/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 3.2266 - acc: 0.2574 - val_loss: 4.9405 - val_acc: 0.0556\n",
      "Epoch 109/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 3.1972 - acc: 0.2597 - val_loss: 4.9384 - val_acc: 0.0561\n",
      "Epoch 110/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 3.1729 - acc: 0.2644 - val_loss: 4.8935 - val_acc: 0.0600\n",
      "Epoch 111/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 3.1467 - acc: 0.2688 - val_loss: 5.0048 - val_acc: 0.0585\n",
      "Epoch 112/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 3.1286 - acc: 0.2768 - val_loss: 4.9442 - val_acc: 0.0580\n",
      "Epoch 113/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 3.1215 - acc: 0.2755 - val_loss: 4.9395 - val_acc: 0.0580\n",
      "Epoch 114/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 3.0655 - acc: 0.2935 - val_loss: 4.9655 - val_acc: 0.0531\n",
      "Epoch 115/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 3.0720 - acc: 0.2865 - val_loss: 4.9938 - val_acc: 0.0575\n",
      "Epoch 116/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 3.0413 - acc: 0.2925 - val_loss: 4.9256 - val_acc: 0.0625\n",
      "Epoch 117/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 2.9990 - acc: 0.3069 - val_loss: 4.9356 - val_acc: 0.0575\n",
      "Epoch 118/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 2.9924 - acc: 0.3087 - val_loss: 4.9498 - val_acc: 0.0561\n",
      "Epoch 119/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 2.9605 - acc: 0.3079 - val_loss: 4.9314 - val_acc: 0.0615\n",
      "Epoch 120/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 2.9227 - acc: 0.3228 - val_loss: 4.9677 - val_acc: 0.0645\n",
      "Epoch 121/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6112/6112 [==============================] - 9s 1ms/step - loss: 2.9014 - acc: 0.3236 - val_loss: 4.9347 - val_acc: 0.0570\n",
      "Epoch 122/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 2.8877 - acc: 0.3238 - val_loss: 5.0138 - val_acc: 0.0590\n",
      "Epoch 123/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 2.8667 - acc: 0.3276 - val_loss: 4.9774 - val_acc: 0.0620\n",
      "Epoch 124/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 2.8234 - acc: 0.3424 - val_loss: 4.9613 - val_acc: 0.0595\n",
      "Epoch 125/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 2.8249 - acc: 0.3357 - val_loss: 4.9805 - val_acc: 0.0635\n",
      "Epoch 126/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 2.7750 - acc: 0.3511 - val_loss: 4.9664 - val_acc: 0.0635\n",
      "Epoch 127/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 2.7697 - acc: 0.3531 - val_loss: 5.0085 - val_acc: 0.0590\n",
      "Epoch 128/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 2.7411 - acc: 0.3559 - val_loss: 4.9620 - val_acc: 0.0704\n",
      "Epoch 129/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 2.7296 - acc: 0.3670 - val_loss: 5.0799 - val_acc: 0.06153 - acc: 0.3\n",
      "Epoch 130/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 2.6812 - acc: 0.3717 - val_loss: 5.1001 - val_acc: 0.0610\n",
      "Epoch 131/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 2.6731 - acc: 0.3735 - val_loss: 5.0430 - val_acc: 0.0610\n",
      "Epoch 132/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 2.6591 - acc: 0.3775 - val_loss: 4.9919 - val_acc: 0.0665\n",
      "Epoch 133/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 2.6283 - acc: 0.3801 - val_loss: 5.0127 - val_acc: 0.0694\n",
      "Epoch 134/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 2.6033 - acc: 0.3887 - val_loss: 5.0304 - val_acc: 0.0655\n",
      "Epoch 135/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 2.5745 - acc: 0.3901 - val_loss: 5.0511 - val_acc: 0.0625\n",
      "Epoch 136/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 2.5524 - acc: 0.3925 - val_loss: 5.0468 - val_acc: 0.0630\n",
      "Epoch 137/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 2.5169 - acc: 0.4064 - val_loss: 5.0469 - val_acc: 0.0660\n",
      "Epoch 138/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 2.5046 - acc: 0.4128 - val_loss: 5.0220 - val_acc: 0.0680\n",
      "Epoch 139/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 2.4783 - acc: 0.4185 - val_loss: 5.0543 - val_acc: 0.0729\n",
      "Epoch 140/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 2.4487 - acc: 0.4265 - val_loss: 5.1159 - val_acc: 0.0699\n",
      "Epoch 141/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 2.4312 - acc: 0.4239 - val_loss: 5.0804 - val_acc: 0.0680\n",
      "Epoch 142/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 2.3938 - acc: 0.4393 - val_loss: 5.0650 - val_acc: 0.0670\n",
      "Epoch 143/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 2.3644 - acc: 0.4481 - val_loss: 5.0979 - val_acc: 0.0630\n",
      "Epoch 144/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 2.3520 - acc: 0.4416 - val_loss: 5.0767 - val_acc: 0.0655\n",
      "Epoch 145/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 2.3441 - acc: 0.4483 - val_loss: 5.0364 - val_acc: 0.0660\n",
      "Epoch 146/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 2.2875 - acc: 0.4562 - val_loss: 5.1056 - val_acc: 0.0665\n",
      "Epoch 147/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 2.3043 - acc: 0.4508 - val_loss: 5.0881 - val_acc: 0.0670\n",
      "Epoch 148/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 2.2613 - acc: 0.4653 - val_loss: 5.0957 - val_acc: 0.0675\n",
      "Epoch 149/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 2.2376 - acc: 0.4712 - val_loss: 5.1115 - val_acc: 0.0570\n",
      "Epoch 150/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 2.2326 - acc: 0.4697 - val_loss: 5.1284 - val_acc: 0.0640\n",
      "Epoch 151/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 2.2005 - acc: 0.4848 - val_loss: 5.1588 - val_acc: 0.0625\n",
      "Epoch 152/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 2.1506 - acc: 0.4933 - val_loss: 5.1573 - val_acc: 0.0670\n",
      "Epoch 153/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 2.1624 - acc: 0.4872 - val_loss: 5.1430 - val_acc: 0.0685\n",
      "Epoch 154/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 2.1059 - acc: 0.5057 - val_loss: 5.1494 - val_acc: 0.0685\n",
      "Epoch 155/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 2.0947 - acc: 0.5095 - val_loss: 5.1656 - val_acc: 0.0685\n",
      "Epoch 156/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 2.0868 - acc: 0.5059 - val_loss: 5.1957 - val_acc: 0.0675\n",
      "Epoch 157/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 2.0558 - acc: 0.5193 - val_loss: 5.2190 - val_acc: 0.0670\n",
      "Epoch 158/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 2.0328 - acc: 0.5237 - val_loss: 5.2208 - val_acc: 0.0670\n",
      "Epoch 159/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 2.0042 - acc: 0.5334 - val_loss: 5.2193 - val_acc: 0.0630\n",
      "Epoch 160/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 1.9865 - acc: 0.5309 - val_loss: 5.1994 - val_acc: 0.0694\n",
      "Epoch 161/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 1.9701 - acc: 0.5362 - val_loss: 5.2037 - val_acc: 0.0655\n",
      "Epoch 162/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 1.9516 - acc: 0.5419 - val_loss: 5.1944 - val_acc: 0.0719\n",
      "Epoch 163/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 1.9190 - acc: 0.5488 - val_loss: 5.2375 - val_acc: 0.0699\n",
      "Epoch 164/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 1.8968 - acc: 0.5558 - val_loss: 5.2830 - val_acc: 0.0689\n",
      "Epoch 165/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 1.8845 - acc: 0.5556 - val_loss: 5.2296 - val_acc: 0.0729\n",
      "Epoch 166/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 1.8546 - acc: 0.5674 - val_loss: 5.2249 - val_acc: 0.0729\n",
      "Epoch 167/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 1.8261 - acc: 0.5797 - val_loss: 5.2483 - val_acc: 0.0724\n",
      "Epoch 168/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 1.7977 - acc: 0.5810 - val_loss: 5.2826 - val_acc: 0.0694\n",
      "Epoch 169/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 1.7706 - acc: 0.5887 - val_loss: 5.3057 - val_acc: 0.0694\n",
      "Epoch 170/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 1.7640 - acc: 0.5885 - val_loss: 5.3535 - val_acc: 0.0689\n",
      "Epoch 171/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 1.7864 - acc: 0.5838 - val_loss: 5.3129 - val_acc: 0.0699\n",
      "Epoch 172/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 1.7152 - acc: 0.6052 - val_loss: 5.2846 - val_acc: 0.0685\n",
      "Epoch 173/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 1.6831 - acc: 0.6243 - val_loss: 5.3332 - val_acc: 0.0714\n",
      "Epoch 174/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 1.6628 - acc: 0.6170 - val_loss: 5.2775 - val_acc: 0.0739\n",
      "Epoch 175/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 1.6667 - acc: 0.6175 - val_loss: 5.3605 - val_acc: 0.0670\n",
      "Epoch 176/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 1.6234 - acc: 0.6306 - val_loss: 5.3470 - val_acc: 0.0729\n",
      "Epoch 177/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 1.6053 - acc: 0.6270 - val_loss: 5.3527 - val_acc: 0.0685\n",
      "Epoch 178/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 1.5922 - acc: 0.6311 - val_loss: 5.3657 - val_acc: 0.0704\n",
      "Epoch 179/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 1.5607 - acc: 0.6412 - val_loss: 5.3893 - val_acc: 0.0699\n",
      "Epoch 180/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6112/6112 [==============================] - 7s 1ms/step - loss: 1.5421 - acc: 0.6491 - val_loss: 5.3490 - val_acc: 0.0729\n",
      "Epoch 181/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 1.5328 - acc: 0.6517 - val_loss: 5.3726 - val_acc: 0.0665\n",
      "Epoch 182/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 1.4976 - acc: 0.6620 - val_loss: 5.4359 - val_acc: 0.0680\n",
      "Epoch 183/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 1.4798 - acc: 0.6688 - val_loss: 5.4336 - val_acc: 0.0685\n",
      "Epoch 184/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 1.4662 - acc: 0.6703 - val_loss: 5.4320 - val_acc: 0.0685\n",
      "Epoch 185/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 1.4492 - acc: 0.6760 - val_loss: 5.4178 - val_acc: 0.0699\n",
      "Epoch 186/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 1.4136 - acc: 0.6878 - val_loss: 5.4257 - val_acc: 0.0699\n",
      "Epoch 187/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 1.4038 - acc: 0.6828 - val_loss: 5.4647 - val_acc: 0.0704\n",
      "Epoch 188/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 1.3687 - acc: 0.6950 - val_loss: 5.4352 - val_acc: 0.0754\n",
      "Epoch 189/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 1.3591 - acc: 0.6957 - val_loss: 5.5597 - val_acc: 0.0719\n",
      "Epoch 190/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 1.3646 - acc: 0.6921 - val_loss: 5.4993 - val_acc: 0.0660\n",
      "Epoch 191/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 1.3284 - acc: 0.7057 - val_loss: 5.5473 - val_acc: 0.0685\n",
      "Epoch 192/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 1.3209 - acc: 0.7088 - val_loss: 5.5488 - val_acc: 0.0709\n",
      "Epoch 193/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 1.3116 - acc: 0.7093 - val_loss: 5.4995 - val_acc: 0.0719\n",
      "Epoch 194/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 1.2827 - acc: 0.7138 - val_loss: 5.4935 - val_acc: 0.0764\n",
      "Epoch 195/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 1.2527 - acc: 0.7269 - val_loss: 5.5203 - val_acc: 0.0739\n",
      "Epoch 196/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 1.2418 - acc: 0.7294 - val_loss: 5.5753 - val_acc: 0.0694\n",
      "Epoch 197/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 1.2188 - acc: 0.7313 - val_loss: 5.5750 - val_acc: 0.0704\n",
      "Epoch 198/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 1.2113 - acc: 0.7371 - val_loss: 5.5404 - val_acc: 0.0699\n",
      "Epoch 199/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 1.1829 - acc: 0.7433 - val_loss: 5.5191 - val_acc: 0.0709\n",
      "Epoch 200/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 1.1552 - acc: 0.7531 - val_loss: 5.6140 - val_acc: 0.0709\n",
      "Epoch 201/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 1.1432 - acc: 0.7603 - val_loss: 5.5822 - val_acc: 0.0709\n",
      "Epoch 202/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 1.1390 - acc: 0.7546 - val_loss: 5.6476 - val_acc: 0.0709\n",
      "Epoch 203/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 1.0992 - acc: 0.7667 - val_loss: 5.6191 - val_acc: 0.0685\n",
      "Epoch 204/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 1.0872 - acc: 0.7703 - val_loss: 5.6012 - val_acc: 0.0709\n",
      "Epoch 205/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 1.0819 - acc: 0.7721 - val_loss: 5.6654 - val_acc: 0.0680\n",
      "Epoch 206/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 1.0519 - acc: 0.7755 - val_loss: 5.6642 - val_acc: 0.0714\n",
      "Epoch 207/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 1.0566 - acc: 0.7739 - val_loss: 5.6888 - val_acc: 0.0704\n",
      "Epoch 208/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 1.0090 - acc: 0.7978 - val_loss: 5.6477 - val_acc: 0.0714\n",
      "Epoch 209/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 1.0226 - acc: 0.7852 - val_loss: 5.6670 - val_acc: 0.0734\n",
      "Epoch 210/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 1.0052 - acc: 0.7876 - val_loss: 5.7121 - val_acc: 0.0670\n",
      "Epoch 211/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.9848 - acc: 0.7950 - val_loss: 5.7162 - val_acc: 0.0704\n",
      "Epoch 212/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.9603 - acc: 0.7996 - val_loss: 5.7337 - val_acc: 0.0699\n",
      "Epoch 213/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.9375 - acc: 0.8105 - val_loss: 5.7172 - val_acc: 0.0739\n",
      "Epoch 214/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.9274 - acc: 0.8081 - val_loss: 5.7173 - val_acc: 0.0709\n",
      "Epoch 215/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.9131 - acc: 0.8171 - val_loss: 5.7852 - val_acc: 0.0724\n",
      "Epoch 216/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.8929 - acc: 0.8212 - val_loss: 5.7529 - val_acc: 0.0724\n",
      "Epoch 217/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.8892 - acc: 0.8231 - val_loss: 5.7790 - val_acc: 0.0680\n",
      "Epoch 218/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.8742 - acc: 0.8271 - val_loss: 5.8293 - val_acc: 0.0759\n",
      "Epoch 219/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.8668 - acc: 0.8266 - val_loss: 5.8651 - val_acc: 0.0665\n",
      "Epoch 220/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.8316 - acc: 0.8390 - val_loss: 5.8437 - val_acc: 0.0699\n",
      "Epoch 221/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.8292 - acc: 0.8367 - val_loss: 5.8893 - val_acc: 0.0714\n",
      "Epoch 222/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.8270 - acc: 0.8377 - val_loss: 5.8422 - val_acc: 0.0724\n",
      "Epoch 223/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.8027 - acc: 0.8478 - val_loss: 5.8419 - val_acc: 0.0744\n",
      "Epoch 224/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.7805 - acc: 0.8496 - val_loss: 5.8806 - val_acc: 0.0670\n",
      "Epoch 225/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.7689 - acc: 0.8541 - val_loss: 5.8746 - val_acc: 0.0675\n",
      "Epoch 226/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.7570 - acc: 0.8541 - val_loss: 5.8868 - val_acc: 0.0665\n",
      "Epoch 227/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.7371 - acc: 0.8598 - val_loss: 5.8760 - val_acc: 0.0754\n",
      "Epoch 228/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.7193 - acc: 0.8706 - val_loss: 5.8969 - val_acc: 0.0744\n",
      "Epoch 229/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.7075 - acc: 0.8711 - val_loss: 5.9605 - val_acc: 0.0749\n",
      "Epoch 230/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.7156 - acc: 0.8650 - val_loss: 5.9431 - val_acc: 0.0685\n",
      "Epoch 231/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.6867 - acc: 0.8747 - val_loss: 5.9788 - val_acc: 0.0719\n",
      "Epoch 232/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.6736 - acc: 0.8776 - val_loss: 5.9665 - val_acc: 0.0709\n",
      "Epoch 233/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.6691 - acc: 0.8817 - val_loss: 6.0141 - val_acc: 0.0739\n",
      "Epoch 234/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.6533 - acc: 0.8876 - val_loss: 5.9732 - val_acc: 0.0699\n",
      "Epoch 235/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.6341 - acc: 0.8881 - val_loss: 5.9723 - val_acc: 0.0749\n",
      "Epoch 236/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.6313 - acc: 0.8940 - val_loss: 5.9844 - val_acc: 0.0729\n",
      "Epoch 237/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.6174 - acc: 0.8860 - val_loss: 6.0989 - val_acc: 0.0665\n",
      "Epoch 238/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.6188 - acc: 0.8894 - val_loss: 6.0867 - val_acc: 0.0665\n",
      "Epoch 239/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.5969 - acc: 0.8968 - val_loss: 6.0795 - val_acc: 0.0685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.5595 - acc: 0.9089 - val_loss: 6.0469 - val_acc: 0.0759\n",
      "Epoch 241/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.5582 - acc: 0.9038 - val_loss: 6.0729 - val_acc: 0.0685\n",
      "Epoch 242/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.5528 - acc: 0.9079 - val_loss: 6.0909 - val_acc: 0.0670\n",
      "Epoch 243/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.5581 - acc: 0.9059 - val_loss: 6.1133 - val_acc: 0.0704\n",
      "Epoch 244/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.5190 - acc: 0.9185 - val_loss: 6.0666 - val_acc: 0.0714\n",
      "Epoch 245/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.5185 - acc: 0.9198 - val_loss: 6.1767 - val_acc: 0.0699\n",
      "Epoch 246/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.5186 - acc: 0.9161 - val_loss: 6.1393 - val_acc: 0.0729\n",
      "Epoch 247/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.5083 - acc: 0.9136 - val_loss: 6.1712 - val_acc: 0.0689\n",
      "Epoch 248/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.4966 - acc: 0.9200 - val_loss: 6.1700 - val_acc: 0.0744\n",
      "Epoch 249/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.4954 - acc: 0.9208 - val_loss: 6.2347 - val_acc: 0.0764\n",
      "Epoch 250/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.4813 - acc: 0.9231 - val_loss: 6.2354 - val_acc: 0.0729\n",
      "Epoch 251/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.4733 - acc: 0.9234 - val_loss: 6.2365 - val_acc: 0.0729\n",
      "Epoch 252/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.4593 - acc: 0.9296 - val_loss: 6.2591 - val_acc: 0.0754\n",
      "Epoch 253/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.4599 - acc: 0.9303 - val_loss: 6.1991 - val_acc: 0.0754\n",
      "Epoch 254/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.4296 - acc: 0.9386 - val_loss: 6.2258 - val_acc: 0.0660\n",
      "Epoch 255/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.4327 - acc: 0.9377 - val_loss: 6.2172 - val_acc: 0.0739\n",
      "Epoch 256/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.4119 - acc: 0.9445 - val_loss: 6.2559 - val_acc: 0.0719\n",
      "Epoch 257/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.4113 - acc: 0.9390 - val_loss: 6.2812 - val_acc: 0.0764\n",
      "Epoch 258/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.3909 - acc: 0.9488 - val_loss: 6.2716 - val_acc: 0.0794\n",
      "Epoch 259/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.3896 - acc: 0.9488 - val_loss: 6.3420 - val_acc: 0.0729\n",
      "Epoch 260/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.3891 - acc: 0.9472 - val_loss: 6.3626 - val_acc: 0.0709\n",
      "Epoch 261/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.3892 - acc: 0.9462 - val_loss: 6.3276 - val_acc: 0.0704\n",
      "Epoch 262/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.3637 - acc: 0.9557 - val_loss: 6.3087 - val_acc: 0.0714\n",
      "Epoch 263/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.3554 - acc: 0.9547 - val_loss: 6.3833 - val_acc: 0.0724\n",
      "Epoch 264/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.3463 - acc: 0.9589 - val_loss: 6.3181 - val_acc: 0.0784\n",
      "Epoch 265/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.3483 - acc: 0.9555 - val_loss: 6.3749 - val_acc: 0.0694\n",
      "Epoch 266/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.3446 - acc: 0.9501 - val_loss: 6.4065 - val_acc: 0.0739\n",
      "Epoch 267/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.3364 - acc: 0.9555 - val_loss: 6.4500 - val_acc: 0.0694\n",
      "Epoch 268/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.3312 - acc: 0.9588 - val_loss: 6.3889 - val_acc: 0.0749\n",
      "Epoch 269/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.3207 - acc: 0.9635 - val_loss: 6.4224 - val_acc: 0.0724\n",
      "Epoch 270/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.3210 - acc: 0.9614 - val_loss: 6.4647 - val_acc: 0.0680\n",
      "Epoch 271/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.3139 - acc: 0.9609 - val_loss: 6.4628 - val_acc: 0.0719\n",
      "Epoch 272/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.3007 - acc: 0.9645 - val_loss: 6.4233 - val_acc: 0.0709\n",
      "Epoch 273/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.2922 - acc: 0.9691 - val_loss: 6.5024 - val_acc: 0.0719\n",
      "Epoch 274/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.2841 - acc: 0.9689 - val_loss: 6.4634 - val_acc: 0.0744\n",
      "Epoch 275/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.2938 - acc: 0.9663 - val_loss: 6.5314 - val_acc: 0.0724\n",
      "Epoch 276/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.2806 - acc: 0.9692 - val_loss: 6.5212 - val_acc: 0.0749\n",
      "Epoch 277/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.2640 - acc: 0.9701 - val_loss: 6.5035 - val_acc: 0.0704\n",
      "Epoch 278/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.2716 - acc: 0.9707 - val_loss: 6.5162 - val_acc: 0.0754\n",
      "Epoch 279/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.2563 - acc: 0.9717 - val_loss: 6.5152 - val_acc: 0.0734\n",
      "Epoch 280/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.2484 - acc: 0.9740 - val_loss: 6.5557 - val_acc: 0.0744\n",
      "Epoch 281/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.2415 - acc: 0.9727 - val_loss: 6.6006 - val_acc: 0.0739\n",
      "Epoch 282/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.2453 - acc: 0.9756 - val_loss: 6.5744 - val_acc: 0.0714\n",
      "Epoch 283/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.2350 - acc: 0.9764 - val_loss: 6.6520 - val_acc: 0.0754\n",
      "Epoch 284/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.2316 - acc: 0.9750 - val_loss: 6.6445 - val_acc: 0.0719\n",
      "Epoch 285/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.2217 - acc: 0.9792 - val_loss: 6.6130 - val_acc: 0.0769\n",
      "Epoch 286/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.2281 - acc: 0.9769 - val_loss: 6.6064 - val_acc: 0.0729\n",
      "Epoch 287/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.2224 - acc: 0.9784 - val_loss: 6.6036 - val_acc: 0.0823\n",
      "Epoch 288/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.2124 - acc: 0.9795 - val_loss: 6.6943 - val_acc: 0.0784\n",
      "Epoch 289/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.2114 - acc: 0.9813 - val_loss: 6.6767 - val_acc: 0.0724\n",
      "Epoch 290/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.2039 - acc: 0.9813 - val_loss: 6.7294 - val_acc: 0.0744\n",
      "Epoch 291/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.2082 - acc: 0.9787 - val_loss: 6.6985 - val_acc: 0.0714\n",
      "Epoch 292/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.2039 - acc: 0.9809 - val_loss: 6.6943 - val_acc: 0.0744\n",
      "Epoch 293/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.1983 - acc: 0.9813 - val_loss: 6.7605 - val_acc: 0.0734\n",
      "Epoch 294/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.1961 - acc: 0.9822 - val_loss: 6.6921 - val_acc: 0.0774\n",
      "Epoch 295/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.1846 - acc: 0.9835 - val_loss: 6.7436 - val_acc: 0.0724\n",
      "Epoch 296/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.1793 - acc: 0.9866 - val_loss: 6.7320 - val_acc: 0.0724\n",
      "Epoch 297/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.1743 - acc: 0.9871 - val_loss: 6.7826 - val_acc: 0.0714\n",
      "Epoch 298/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.1732 - acc: 0.9859 - val_loss: 6.7475 - val_acc: 0.0779\n",
      "Epoch 299/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.1664 - acc: 0.9879 - val_loss: 6.7348 - val_acc: 0.0759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.1579 - acc: 0.9864 - val_loss: 6.7381 - val_acc: 0.0764\n",
      "Epoch 301/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.1668 - acc: 0.9845 - val_loss: 6.7656 - val_acc: 0.0744\n",
      "Epoch 302/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.1670 - acc: 0.9845 - val_loss: 6.8111 - val_acc: 0.0774\n",
      "Epoch 303/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.1535 - acc: 0.9890 - val_loss: 6.7977 - val_acc: 0.0734\n",
      "Epoch 304/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.1544 - acc: 0.9882 - val_loss: 6.8867 - val_acc: 0.0804\n",
      "Epoch 305/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.1658 - acc: 0.9835 - val_loss: 6.8195 - val_acc: 0.0729\n",
      "Epoch 306/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.1517 - acc: 0.9856 - val_loss: 6.8392 - val_acc: 0.0739\n",
      "Epoch 307/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.1436 - acc: 0.9895 - val_loss: 6.8250 - val_acc: 0.0724\n",
      "Epoch 308/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.1409 - acc: 0.9902 - val_loss: 6.9333 - val_acc: 0.0764\n",
      "Epoch 309/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.1364 - acc: 0.9895 - val_loss: 6.9010 - val_acc: 0.0709\n",
      "Epoch 310/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.1350 - acc: 0.9923 - val_loss: 6.8813 - val_acc: 0.0749\n",
      "Epoch 311/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.1324 - acc: 0.9926 - val_loss: 6.9171 - val_acc: 0.0769\n",
      "Epoch 312/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.1321 - acc: 0.9900 - val_loss: 6.9023 - val_acc: 0.0774\n",
      "Epoch 313/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.1243 - acc: 0.9918 - val_loss: 6.9265 - val_acc: 0.0769\n",
      "Epoch 314/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.1246 - acc: 0.9915 - val_loss: 6.8989 - val_acc: 0.0739\n",
      "Epoch 315/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.1271 - acc: 0.9908 - val_loss: 6.9787 - val_acc: 0.0774\n",
      "Epoch 316/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.1204 - acc: 0.9908 - val_loss: 6.9641 - val_acc: 0.0759\n",
      "Epoch 317/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.1210 - acc: 0.9890 - val_loss: 6.9437 - val_acc: 0.0759\n",
      "Epoch 318/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.1126 - acc: 0.9928 - val_loss: 7.0131 - val_acc: 0.0744\n",
      "Epoch 319/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.1143 - acc: 0.9917 - val_loss: 7.0048 - val_acc: 0.0694\n",
      "Epoch 320/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.1138 - acc: 0.9915 - val_loss: 7.0202 - val_acc: 0.0754\n",
      "Epoch 321/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.1090 - acc: 0.9918 - val_loss: 7.0040 - val_acc: 0.0739\n",
      "Epoch 322/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.1054 - acc: 0.9915 - val_loss: 7.0032 - val_acc: 0.0685\n",
      "Epoch 323/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.1051 - acc: 0.9948 - val_loss: 6.9984 - val_acc: 0.0729\n",
      "Epoch 324/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.1090 - acc: 0.9908 - val_loss: 7.0263 - val_acc: 0.0769\n",
      "Epoch 325/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.1036 - acc: 0.9928 - val_loss: 7.0695 - val_acc: 0.0794\n",
      "Epoch 326/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.1007 - acc: 0.9925 - val_loss: 7.0328 - val_acc: 0.0804\n",
      "Epoch 327/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0913 - acc: 0.9948 - val_loss: 7.0553 - val_acc: 0.0749\n",
      "Epoch 328/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0986 - acc: 0.9928 - val_loss: 7.1158 - val_acc: 0.0704\n",
      "Epoch 329/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0938 - acc: 0.9933 - val_loss: 7.0870 - val_acc: 0.0774\n",
      "Epoch 330/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0938 - acc: 0.9933 - val_loss: 7.1434 - val_acc: 0.0769\n",
      "Epoch 331/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0946 - acc: 0.9925 - val_loss: 7.1830 - val_acc: 0.0739\n",
      "Epoch 332/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0955 - acc: 0.9941 - val_loss: 7.1437 - val_acc: 0.0744\n",
      "Epoch 333/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0954 - acc: 0.9938 - val_loss: 7.1282 - val_acc: 0.0774\n",
      "Epoch 334/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0891 - acc: 0.9935 - val_loss: 7.1873 - val_acc: 0.0729\n",
      "Epoch 335/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0904 - acc: 0.9925 - val_loss: 7.1387 - val_acc: 0.0694\n",
      "Epoch 336/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0943 - acc: 0.9928 - val_loss: 7.1308 - val_acc: 0.0764\n",
      "Epoch 337/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0826 - acc: 0.9938 - val_loss: 7.1569 - val_acc: 0.0714\n",
      "Epoch 338/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0848 - acc: 0.9941 - val_loss: 7.2311 - val_acc: 0.0719\n",
      "Epoch 339/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0782 - acc: 0.9959 - val_loss: 7.1824 - val_acc: 0.0804\n",
      "Epoch 340/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0789 - acc: 0.9964 - val_loss: 7.1711 - val_acc: 0.0789\n",
      "Epoch 341/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0802 - acc: 0.9944 - val_loss: 7.2406 - val_acc: 0.0754\n",
      "Epoch 342/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0719 - acc: 0.9962 - val_loss: 7.2108 - val_acc: 0.0739\n",
      "Epoch 343/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0744 - acc: 0.9948 - val_loss: 7.2429 - val_acc: 0.0774\n",
      "Epoch 344/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0758 - acc: 0.9944 - val_loss: 7.2384 - val_acc: 0.0744\n",
      "Epoch 345/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0745 - acc: 0.9943 - val_loss: 7.2762 - val_acc: 0.0764\n",
      "Epoch 346/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0729 - acc: 0.9943 - val_loss: 7.2326 - val_acc: 0.0779\n",
      "Epoch 347/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0727 - acc: 0.9954 - val_loss: 7.2615 - val_acc: 0.0809\n",
      "Epoch 348/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0632 - acc: 0.9966 - val_loss: 7.3054 - val_acc: 0.0739\n",
      "Epoch 349/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0583 - acc: 0.9977 - val_loss: 7.3420 - val_acc: 0.0759\n",
      "Epoch 350/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0669 - acc: 0.9948 - val_loss: 7.2787 - val_acc: 0.0744\n",
      "Epoch 351/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0688 - acc: 0.9944 - val_loss: 7.2644 - val_acc: 0.0744\n",
      "Epoch 352/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0676 - acc: 0.9946 - val_loss: 7.3389 - val_acc: 0.0739\n",
      "Epoch 353/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0689 - acc: 0.9946 - val_loss: 7.3584 - val_acc: 0.0689\n",
      "Epoch 354/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0635 - acc: 0.9948 - val_loss: 7.3523 - val_acc: 0.0754\n",
      "Epoch 355/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0641 - acc: 0.9957 - val_loss: 7.3806 - val_acc: 0.0719\n",
      "Epoch 356/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0618 - acc: 0.9962 - val_loss: 7.4799 - val_acc: 0.0714\n",
      "Epoch 357/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0618 - acc: 0.9946 - val_loss: 7.3866 - val_acc: 0.0729\n",
      "Epoch 358/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0938 - acc: 0.9885 - val_loss: 7.3701 - val_acc: 0.0739\n",
      "Epoch 359/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0729 - acc: 0.9936 - val_loss: 7.3031 - val_acc: 0.0754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 360/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0525 - acc: 0.9961 - val_loss: 7.3914 - val_acc: 0.0759\n",
      "Epoch 361/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0510 - acc: 0.9967 - val_loss: 7.3928 - val_acc: 0.0744\n",
      "Epoch 362/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0519 - acc: 0.9967 - val_loss: 7.3959 - val_acc: 0.0739\n",
      "Epoch 363/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0508 - acc: 0.9971 - val_loss: 7.4421 - val_acc: 0.0724\n",
      "Epoch 364/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0515 - acc: 0.9964 - val_loss: 7.4192 - val_acc: 0.0769\n",
      "Epoch 365/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0517 - acc: 0.9971 - val_loss: 7.4177 - val_acc: 0.0744\n",
      "Epoch 366/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0532 - acc: 0.9967 - val_loss: 7.4510 - val_acc: 0.0744\n",
      "Epoch 367/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0547 - acc: 0.9959 - val_loss: 7.4532 - val_acc: 0.0764\n",
      "Epoch 368/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0515 - acc: 0.9962 - val_loss: 7.5235 - val_acc: 0.0734\n",
      "Epoch 369/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0515 - acc: 0.9967 - val_loss: 7.4967 - val_acc: 0.0764\n",
      "Epoch 370/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0559 - acc: 0.9961 - val_loss: 7.4371 - val_acc: 0.0754\n",
      "Epoch 371/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0456 - acc: 0.9972 - val_loss: 7.4712 - val_acc: 0.0789\n",
      "Epoch 372/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0444 - acc: 0.9980 - val_loss: 7.5861 - val_acc: 0.0744\n",
      "Epoch 373/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0456 - acc: 0.9974 - val_loss: 7.5405 - val_acc: 0.0789\n",
      "Epoch 374/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0447 - acc: 0.9969 - val_loss: 7.5025 - val_acc: 0.0739\n",
      "Epoch 375/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0432 - acc: 0.9969 - val_loss: 7.5027 - val_acc: 0.0754\n",
      "Epoch 376/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0453 - acc: 0.9954 - val_loss: 7.5015 - val_acc: 0.0769\n",
      "Epoch 377/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0431 - acc: 0.9974 - val_loss: 7.5387 - val_acc: 0.0779\n",
      "Epoch 378/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0441 - acc: 0.9974 - val_loss: 7.5218 - val_acc: 0.0759\n",
      "Epoch 379/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0470 - acc: 0.9967 - val_loss: 7.5431 - val_acc: 0.0724\n",
      "Epoch 380/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0468 - acc: 0.9957 - val_loss: 7.5807 - val_acc: 0.0754\n",
      "Epoch 381/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0459 - acc: 0.9971 - val_loss: 7.5808 - val_acc: 0.0843\n",
      "Epoch 382/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0435 - acc: 0.9971 - val_loss: 7.5555 - val_acc: 0.0799\n",
      "Epoch 383/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0430 - acc: 0.9971 - val_loss: 7.6245 - val_acc: 0.0739\n",
      "Epoch 384/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0455 - acc: 0.9959 - val_loss: 7.5737 - val_acc: 0.0709\n",
      "Epoch 385/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0476 - acc: 0.9962 - val_loss: 7.5538 - val_acc: 0.0734\n",
      "Epoch 386/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0437 - acc: 0.9967 - val_loss: 7.5503 - val_acc: 0.0779\n",
      "Epoch 387/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0364 - acc: 0.9979 - val_loss: 7.6326 - val_acc: 0.0749\n",
      "Epoch 388/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0353 - acc: 0.9977 - val_loss: 7.6031 - val_acc: 0.0749\n",
      "Epoch 389/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0381 - acc: 0.9966 - val_loss: 7.5874 - val_acc: 0.0779\n",
      "Epoch 390/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0372 - acc: 0.9974 - val_loss: 7.6644 - val_acc: 0.0739\n",
      "Epoch 391/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0378 - acc: 0.9972 - val_loss: 7.6845 - val_acc: 0.0779\n",
      "Epoch 392/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0411 - acc: 0.9962 - val_loss: 7.6266 - val_acc: 0.0739\n",
      "Epoch 393/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0418 - acc: 0.9967 - val_loss: 7.6724 - val_acc: 0.0724\n",
      "Epoch 394/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0391 - acc: 0.9971 - val_loss: 7.6978 - val_acc: 0.0804\n",
      "Epoch 395/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0382 - acc: 0.9972 - val_loss: 7.6872 - val_acc: 0.0759\n",
      "Epoch 396/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0389 - acc: 0.9969 - val_loss: 7.7107 - val_acc: 0.0759\n",
      "Epoch 397/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0398 - acc: 0.9974 - val_loss: 7.7009 - val_acc: 0.0744\n",
      "Epoch 398/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0432 - acc: 0.9959 - val_loss: 7.6730 - val_acc: 0.0774\n",
      "Epoch 399/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0371 - acc: 0.9967 - val_loss: 7.7448 - val_acc: 0.0719\n",
      "Epoch 400/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0372 - acc: 0.9962 - val_loss: 7.6678 - val_acc: 0.0818\n",
      "Epoch 401/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0340 - acc: 0.9972 - val_loss: 7.7316 - val_acc: 0.0804\n",
      "Epoch 402/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0328 - acc: 0.9975 - val_loss: 7.7798 - val_acc: 0.0774\n",
      "Epoch 403/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0276 - acc: 0.9975 - val_loss: 7.7345 - val_acc: 0.0749\n",
      "Epoch 404/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0313 - acc: 0.9979 - val_loss: 7.7023 - val_acc: 0.0764\n",
      "Epoch 405/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0342 - acc: 0.9967 - val_loss: 7.7464 - val_acc: 0.0809\n",
      "Epoch 406/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0316 - acc: 0.9977 - val_loss: 7.7663 - val_acc: 0.0789\n",
      "Epoch 407/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0282 - acc: 0.9975 - val_loss: 7.7715 - val_acc: 0.0764\n",
      "Epoch 408/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0335 - acc: 0.9975 - val_loss: 7.7033 - val_acc: 0.0739\n",
      "Epoch 409/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0367 - acc: 0.9969 - val_loss: 7.7396 - val_acc: 0.0744\n",
      "Epoch 410/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0315 - acc: 0.9979 - val_loss: 7.6939 - val_acc: 0.0809\n",
      "Epoch 411/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0358 - acc: 0.9971 - val_loss: 7.7933 - val_acc: 0.0729\n",
      "Epoch 412/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0352 - acc: 0.9971 - val_loss: 7.7540 - val_acc: 0.0789\n",
      "Epoch 413/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0354 - acc: 0.9971 - val_loss: 7.8269 - val_acc: 0.0714\n",
      "Epoch 414/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0313 - acc: 0.9974 - val_loss: 7.8373 - val_acc: 0.0739\n",
      "Epoch 415/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0315 - acc: 0.9977 - val_loss: 7.7867 - val_acc: 0.0799\n",
      "Epoch 416/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0283 - acc: 0.9971 - val_loss: 7.8644 - val_acc: 0.0744\n",
      "Epoch 417/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0283 - acc: 0.9974 - val_loss: 7.8529 - val_acc: 0.0759\n",
      "Epoch 418/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0373 - acc: 0.9961 - val_loss: 7.7154 - val_acc: 0.0789\n",
      "Epoch 419/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0288 - acc: 0.9971 - val_loss: 7.7932 - val_acc: 0.0794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 420/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0271 - acc: 0.9971 - val_loss: 7.8827 - val_acc: 0.0739\n",
      "Epoch 421/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0274 - acc: 0.9975 - val_loss: 7.8983 - val_acc: 0.0759\n",
      "Epoch 422/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0281 - acc: 0.9971 - val_loss: 7.8537 - val_acc: 0.0744\n",
      "Epoch 423/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0254 - acc: 0.9975 - val_loss: 7.8629 - val_acc: 0.0779\n",
      "Epoch 424/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0314 - acc: 0.9969 - val_loss: 7.8009 - val_acc: 0.0744\n",
      "Epoch 425/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0322 - acc: 0.9971 - val_loss: 7.8726 - val_acc: 0.0818\n",
      "Epoch 426/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0289 - acc: 0.9966 - val_loss: 7.9145 - val_acc: 0.0809\n",
      "Epoch 427/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0210 - acc: 0.9987 - val_loss: 7.9905 - val_acc: 0.0749\n",
      "Epoch 428/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0239 - acc: 0.9979 - val_loss: 7.8565 - val_acc: 0.0784\n",
      "Epoch 429/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0243 - acc: 0.9972 - val_loss: 7.8597 - val_acc: 0.0774\n",
      "Epoch 430/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0225 - acc: 0.9975 - val_loss: 7.8732 - val_acc: 0.0828\n",
      "Epoch 431/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0234 - acc: 0.9979 - val_loss: 7.9188 - val_acc: 0.0794\n",
      "Epoch 432/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0212 - acc: 0.9975 - val_loss: 7.9380 - val_acc: 0.0794\n",
      "Epoch 433/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0245 - acc: 0.9974 - val_loss: 7.8928 - val_acc: 0.0779\n",
      "Epoch 434/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0253 - acc: 0.9974 - val_loss: 7.9142 - val_acc: 0.0774\n",
      "Epoch 435/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0242 - acc: 0.9980 - val_loss: 8.0885 - val_acc: 0.0714\n",
      "Epoch 436/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0312 - acc: 0.9961 - val_loss: 7.9222 - val_acc: 0.0744\n",
      "Epoch 437/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0311 - acc: 0.9964 - val_loss: 7.9916 - val_acc: 0.0754\n",
      "Epoch 438/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0206 - acc: 0.9977 - val_loss: 7.9599 - val_acc: 0.0818\n",
      "Epoch 439/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0210 - acc: 0.9977 - val_loss: 7.9825 - val_acc: 0.0764\n",
      "Epoch 440/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0228 - acc: 0.9975 - val_loss: 7.9731 - val_acc: 0.0774\n",
      "Epoch 441/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0261 - acc: 0.9971 - val_loss: 7.9376 - val_acc: 0.0789\n",
      "Epoch 442/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0232 - acc: 0.9975 - val_loss: 7.9597 - val_acc: 0.0799\n",
      "Epoch 443/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0292 - acc: 0.9967 - val_loss: 7.9720 - val_acc: 0.0769\n",
      "Epoch 444/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0417 - acc: 0.9953 - val_loss: 8.1618 - val_acc: 0.0689\n",
      "Epoch 445/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0301 - acc: 0.9972 - val_loss: 7.9829 - val_acc: 0.0818\n",
      "Epoch 446/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0209 - acc: 0.9975 - val_loss: 8.0608 - val_acc: 0.0739\n",
      "Epoch 447/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0178 - acc: 0.9984 - val_loss: 8.0220 - val_acc: 0.0779\n",
      "Epoch 448/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0171 - acc: 0.9982 - val_loss: 8.0586 - val_acc: 0.0699\n",
      "Epoch 449/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0180 - acc: 0.9984 - val_loss: 7.9447 - val_acc: 0.0784\n",
      "Epoch 450/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0206 - acc: 0.9971 - val_loss: 7.9715 - val_acc: 0.0794\n",
      "Epoch 451/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0299 - acc: 0.9969 - val_loss: 8.0590 - val_acc: 0.0779\n",
      "Epoch 452/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0610 - acc: 0.9903 - val_loss: 7.9974 - val_acc: 0.0719\n",
      "Epoch 453/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0268 - acc: 0.9969 - val_loss: 8.0669 - val_acc: 0.0764\n",
      "Epoch 454/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0175 - acc: 0.9980 - val_loss: 7.9901 - val_acc: 0.0769\n",
      "Epoch 455/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0174 - acc: 0.9979 - val_loss: 8.0884 - val_acc: 0.0729\n",
      "Epoch 456/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0164 - acc: 0.9980 - val_loss: 8.0751 - val_acc: 0.0764\n",
      "Epoch 457/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0171 - acc: 0.9975 - val_loss: 8.0760 - val_acc: 0.0789\n",
      "Epoch 458/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0159 - acc: 0.9980 - val_loss: 8.0691 - val_acc: 0.0799\n",
      "Epoch 459/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0161 - acc: 0.9980 - val_loss: 8.1134 - val_acc: 0.0789\n",
      "Epoch 460/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0155 - acc: 0.9979 - val_loss: 8.1427 - val_acc: 0.0744\n",
      "Epoch 461/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0173 - acc: 0.9975 - val_loss: 8.1513 - val_acc: 0.0739\n",
      "Epoch 462/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0157 - acc: 0.9977 - val_loss: 8.1380 - val_acc: 0.0734\n",
      "Epoch 463/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0163 - acc: 0.9979 - val_loss: 8.0889 - val_acc: 0.0823\n",
      "Epoch 464/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0177 - acc: 0.9977 - val_loss: 8.0881 - val_acc: 0.0719\n",
      "Epoch 465/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0183 - acc: 0.9979 - val_loss: 8.1032 - val_acc: 0.0724\n",
      "Epoch 466/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0212 - acc: 0.9962 - val_loss: 8.1885 - val_acc: 0.0754\n",
      "Epoch 467/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0212 - acc: 0.9974 - val_loss: 8.1332 - val_acc: 0.0764\n",
      "Epoch 468/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0182 - acc: 0.9979 - val_loss: 8.1245 - val_acc: 0.0843\n",
      "Epoch 469/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0165 - acc: 0.9979 - val_loss: 8.1124 - val_acc: 0.0813\n",
      "Epoch 470/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0150 - acc: 0.9979 - val_loss: 8.1360 - val_acc: 0.0804\n",
      "Epoch 471/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0146 - acc: 0.9985 - val_loss: 8.1724 - val_acc: 0.0769\n",
      "Epoch 472/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0171 - acc: 0.9975 - val_loss: 8.0974 - val_acc: 0.0823\n",
      "Epoch 473/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0184 - acc: 0.9982 - val_loss: 8.1457 - val_acc: 0.0749\n",
      "Epoch 474/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0161 - acc: 0.9985 - val_loss: 8.1529 - val_acc: 0.0724\n",
      "Epoch 475/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0142 - acc: 0.9984 - val_loss: 8.2412 - val_acc: 0.0774\n",
      "Epoch 476/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.1856 - acc: 0.9524 - val_loss: 7.9068 - val_acc: 0.0714\n",
      "Epoch 477/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0261 - acc: 0.9971 - val_loss: 8.0616 - val_acc: 0.0774\n",
      "Epoch 478/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0158 - acc: 0.9977 - val_loss: 8.0634 - val_acc: 0.0749\n",
      "Epoch 479/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0156 - acc: 0.9979 - val_loss: 8.0684 - val_acc: 0.0759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 480/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0129 - acc: 0.9982 - val_loss: 8.1037 - val_acc: 0.0739\n",
      "Epoch 481/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0125 - acc: 0.9982 - val_loss: 8.1169 - val_acc: 0.0764\n",
      "Epoch 482/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0114 - acc: 0.9979 - val_loss: 8.1255 - val_acc: 0.0759\n",
      "Epoch 483/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0100 - acc: 0.9982 - val_loss: 8.1712 - val_acc: 0.0779\n",
      "Epoch 484/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0098 - acc: 0.9987 - val_loss: 8.1574 - val_acc: 0.0784\n",
      "Epoch 485/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0122 - acc: 0.9980 - val_loss: 8.1131 - val_acc: 0.0744\n",
      "Epoch 486/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0121 - acc: 0.9980 - val_loss: 8.1661 - val_acc: 0.0789\n",
      "Epoch 487/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0102 - acc: 0.9985 - val_loss: 8.1882 - val_acc: 0.0813\n",
      "Epoch 488/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0101 - acc: 0.9984 - val_loss: 8.1994 - val_acc: 0.0744\n",
      "Epoch 489/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0113 - acc: 0.9982 - val_loss: 8.1626 - val_acc: 0.0789\n",
      "Epoch 490/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0139 - acc: 0.9975 - val_loss: 8.1957 - val_acc: 0.0769\n",
      "Epoch 491/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0133 - acc: 0.9977 - val_loss: 8.1912 - val_acc: 0.0848\n",
      "Epoch 492/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0126 - acc: 0.9985 - val_loss: 8.1287 - val_acc: 0.0754\n",
      "Epoch 493/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0140 - acc: 0.9980 - val_loss: 8.2376 - val_acc: 0.0809\n",
      "Epoch 494/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0133 - acc: 0.9977 - val_loss: 8.1934 - val_acc: 0.0794\n",
      "Epoch 495/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0141 - acc: 0.9979 - val_loss: 8.2544 - val_acc: 0.0754\n",
      "Epoch 496/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0089 - acc: 0.9987 - val_loss: 8.2342 - val_acc: 0.0858\n",
      "Epoch 497/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0135 - acc: 0.9980 - val_loss: 8.1926 - val_acc: 0.0804\n",
      "Epoch 498/500\n",
      "6112/6112 [==============================] - 7s 1ms/step - loss: 0.0139 - acc: 0.9975 - val_loss: 8.2437 - val_acc: 0.0804\n",
      "Epoch 499/500\n",
      "6112/6112 [==============================] - 9s 1ms/step - loss: 0.0113 - acc: 0.9982 - val_loss: 8.2503 - val_acc: 0.0789\n",
      "Epoch 500/500\n",
      "6112/6112 [==============================] - 8s 1ms/step - loss: 0.0153 - acc: 0.9977 - val_loss: 8.2617 - val_acc: 0.0774\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: try changing batch size to 32 to see if it still fits in gpu memory\n",
    "\n",
    "# From https://github.com/fchollet/deep-learning-models/issues/13\n",
    "#sgd = keras.optimizers.SGD(lr=0.0005, decay=1e-6, momentum=0.9)\n",
    "sgd = keras.optimizers.Adam(lr=0.0001)\n",
    "\n",
    "def train_top_model_resnet(num_epochs, bottleneck_predictions_train, bottleneck_predictions_validation, train_labels, validation_labels):\n",
    "    \"\"\"\n",
    "    Best params so far:\n",
    "    \n",
    "    SGD with\n",
    "       - 2 4096 dense layers\n",
    "       - Dropout 0.6\n",
    "       - lr=0.0005, decay=1e-6, momentum=0.9\n",
    "       result: loss: 0.1307 - acc: 0.9890 - val_loss: 1.7402 - val_acc: 0.5342\n",
    "    \"\"\"\n",
    "    \n",
    "    top_model = Sequential()\n",
    "    top_model.add(AveragePooling2D(pool_size=(4, 4), data_format='channels_last'))\n",
    "    top_model.add(Flatten())\n",
    "    top_model.add(Dense(4096, activation='sigmoid'))\n",
    "    top_model.add(Dense(4096, activation='sigmoid'))\n",
    "    #top_model.add(Flatten(input_shape=bottleneck_predictions_train.shape[1:]))\n",
    "    #top_model.add(Dense(4096, activation='relu'))\n",
    "    #top_model.add(Dense(4096, activation='relu'))\n",
    "    top_model.add(Dropout(0.5))\n",
    "    top_model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    top_model.compile(\n",
    "        optimizer=sgd,\n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "\n",
    "    top_model.fit(bottleneck_predictions_train, \n",
    "              train_labels,\n",
    "              epochs=num_epochs,\n",
    "              batch_size=64,  # was batch_size=batch_size, experimenting\n",
    "              validation_data=(bottleneck_predictions_validation, validation_labels))\n",
    "    \n",
    "    return top_model\n",
    "\n",
    "\n",
    "top_model = train_top_model_resnet(\n",
    "    num_epochs=500,\n",
    "    bottleneck_predictions_train=training_y_preds_flat_resnet152,\n",
    "    bottleneck_predictions_validation=validation_y_preds_flat_resnet152,\n",
    "    train_labels=training_y_labels_flat_resnet152,\n",
    "    validation_labels=validation_y_labels_flat_resnet152,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning Resnet152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: is the reason this isn't working because the top model wasn't pretrained so it has terrible weights?\n",
    "\n",
    "for layer in base_model_resnet152.layers:\n",
    "        layer.trainable=False\n",
    "        \n",
    "# TODO: don't we want the final layers to be trainable?\n",
    "\n",
    "# TODO: use the top_model created above!!\n",
    "        \n",
    "#x = base_model.output\n",
    "#x = AveragePooling2D(pool_size=(4, 4), data_format='channels_last')(x)\n",
    "#x = Flatten()(x)\n",
    "#x = Dense(4096, activation='relu')(x)\n",
    "#x = Dense(4096, activation='relu')(x)\n",
    "#x = Dropout(0.5)(x)\n",
    "#x = Dense(num_classes, activation='softmax')(x)\n",
    "#model_resnet152 = Model(base_model_resnet152.input, x)\n",
    "\n",
    "combined_model_resnet152 = keras.Model(\n",
    "    input=base_model_resnet152.input, \n",
    "    output=top_model(base_model_resnet152.output)\n",
    ")\n",
    "combined_model_resnet152.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model_resnet152.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model_resnet152.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=50,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
